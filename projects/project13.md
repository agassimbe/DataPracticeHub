# Projet : Pr√©dire le prix d'un bien immobilier gr√¢ce au Machine Learning

## Description

Dans ce projet, nous cherchons √† cr√©er un mod√®le de machine learning capable de pr√©dire le prix des biens immobiliers en France. Nous disposons d'un ensemble de donn√©es r√©elles comprenant diverses caract√©ristiques des propri√©t√©s telles que la taille, le nombre de chambres, l'emplacement g√©ographique, etc. L'objectif est d'exploiter ces informations pour entra√Æner un mod√®le pr√©cis et fiable qui peut estimer le prix d'une propri√©t√© sur la base de ses caract√©ristiques.


## Image
imgs/project13/project13.png

## Instructions

1. **Exploration des Donn√©es (EDA):**
   - Charger les donn√©es dans un notebook (p. ex. Jupyter).
   - Effectuer une exploration des donn√©es pour comprendre la structure, les types de donn√©es, les valeurs manquantes, etc.
   - Visualiser les distributions, les corr√©lations entre les variables et explorer les relations potentielles avec le prix des biens immobiliers.

2. **Feature Engineering:**
   - Identifier les variables pertinentes pour la pr√©diction du prix immobilier.
   - Cr√©er de nouvelles caract√©ristiques potentielles √† partir des donn√©es existantes, telles que la surface habitable par chambre, la distance par rapport aux points d'int√©r√™t, etc.
   - Traiter les valeurs manquantes et les valeurs aberrantes.

3. **Pr√©traitement des Donn√©es:**
   - Effectuer une normalisation ou une standardisation des donn√©es si n√©cessaire.
   - Encoder les variables cat√©gorielles.
   - Diviser les donn√©es en ensembles d'entra√Ænement et de test.

4. **Mod√©lisation:**
   - Choisir un ou plusieurs algorithmes de machine learning appropri√©s (par exemple, r√©gression lin√©aire, random forest, XGBoost, etc.).
   - Entra√Æner les mod√®les sur les donn√©es d'entra√Ænement.
   - √âvaluer les performances des mod√®les √† l'aide de m√©triques telles que l'erreur quadratique moyenne (RMSE), le coefficient de d√©termination (R¬≤), etc.

5. **Optimisation et Validation:**
   - Optimiser les hyperparam√®tres des mod√®les pour am√©liorer les performances.
   - Valider les mod√®les sur l'ensemble de test pour s'assurer de leur g√©n√©ralisation.

6. **Sauvegarde du Mod√®le:**
   - Sauvegarder le mod√®le entra√Æn√© pour une utilisation future.
   - Documenter les √©tapes et les d√©cisions prises tout au long du processus.

7. **Rapport Final:**
   - Pr√©senter les r√©sultats obtenus, y compris les performances du mod√®le, les caract√©ristiques les plus importantes, les d√©fis rencontr√©s, etc.
   - Proposer des recommandations pour l'am√©lioration future du mod√®le ou de la collecte de donn√©es.

En suivant ces √©tapes, l'utilisateur pourra cr√©er un mod√®le de pr√©diction des prix immobiliers en France utilisant des donn√©es r√©elles et des techniques avanc√©es de machine learning.


## Resources

- [Ensemble de donn√©es des Caract√©ristiques](https://drive.google.com/file/d/1aE7TPBJqRgLALYjYoqJaVQfifYF_9GY4/view?usp=sharing)
- [Ensemble de donn√©es des √âtiquettes](https://drive.google.com/file/d/1dkjerJ7u5Tq8XCyTH_6glAxSYVdHpu1w/view?usp=sharing)
- [Regardez le projet en vid√©o](https://youtu.be/3hyxEtBdC_w)
- [Machine Learning par la pratique avec Python](https://www.amazon.fr/dp/B08DV8X9D2?ref_=ast_author_ofdp)
- [Playlist de Vid√©os sur le Machine Learning avec Python](https://www.youtube.com/playlist?list=PLmJWMf9F8euTuNEnfnV-qdaVOOL8cIY9Q)
- [Installation et Configuration d'un environnement Python avec VSC](https://youtu.be/6NYsMiFqH3E)


## Execution du Projet

Pour ce projet, vous pouvez utiliser les packages suivants :

joblib==1.4.2

numpy==1.26.4

pandas==2.2.2

scikit-learn==1.5.0

seaborn==0.13.2

xgboost==2.0.3


üñ• **Notebook pour la r√©alisation du projet**

Cr√©ez un notebook dans votre environnement python. Exemple : *french_real_estate_prediction.ipynb*




```python
import pandas as pd
features = pd.read_csv('X_train_J01Z4CN.csv')
features.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 37368 entries, 0 to 37367
    Data columns (total 27 columns):
     #   Column                       Non-Null Count  Dtype  
    ---  ------                       --------------  -----  
     0   id_annonce                   37368 non-null  int64  
     1   property_type                37368 non-null  object 
     2   approximate_latitude         37368 non-null  float64
     3   approximate_longitude        37368 non-null  float64
     4   city                         37368 non-null  object 
     5   postal_code                  37368 non-null  int64  
     6   size                         36856 non-null  float64
     7   floor                        9743 non-null   float64
     8   land_size                    15581 non-null  float64
     9   energy_performance_value     19068 non-null  float64
     10  energy_performance_category  19068 non-null  object 
     11  ghg_value                    18530 non-null  float64
     12  ghg_category                 18530 non-null  object 
     13  exposition                   9094 non-null   object 
     14  nb_rooms                     35802 non-null  float64
     15  nb_bedrooms                  34635 non-null  float64
     16  nb_bathrooms                 24095 non-null  float64
     17  nb_parking_places            37368 non-null  float64
     18  nb_boxes                     37368 non-null  float64
     19  nb_photos                    37368 non-null  float64
     20  has_a_balcony                37368 non-null  float64
     21  nb_terraces                  37368 non-null  float64
     22  has_a_cellar                 37368 non-null  float64
     23  has_a_garage                 37368 non-null  float64
     24  has_air_conditioning         37368 non-null  float64
     25  last_floor                   37368 non-null  float64
     26  upper_floors                 37368 non-null  float64
    dtypes: float64(20), int64(2), object(5)
    memory usage: 7.7+ MB



```python
features.head()
```


```python
# Target
labels = pd.read_csv('y_train_OXxrJt1.csv')
labels.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 37368 entries, 0 to 37367
    Data columns (total 2 columns):
     #   Column      Non-Null Count  Dtype  
    ---  ------      --------------  -----  
     0   id_annonce  37368 non-null  int64  
     1   price       37368 non-null  float64
    dtypes: float64(1), int64(1)
    memory usage: 584.0 KB



```python
labels.head()
```


```python
df = features.merge(labels, on='id_annonce')
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 37368 entries, 0 to 37367
    Data columns (total 28 columns):
     #   Column                       Non-Null Count  Dtype  
    ---  ------                       --------------  -----  
     0   id_annonce                   37368 non-null  int64  
     1   property_type                37368 non-null  object 
     2   approximate_latitude         37368 non-null  float64
     3   approximate_longitude        37368 non-null  float64
     4   city                         37368 non-null  object 
     5   postal_code                  37368 non-null  int64  
     6   size                         36856 non-null  float64
     7   floor                        9743 non-null   float64
     8   land_size                    15581 non-null  float64
     9   energy_performance_value     19068 non-null  float64
     10  energy_performance_category  19068 non-null  object 
     11  ghg_value                    18530 non-null  float64
     12  ghg_category                 18530 non-null  object 
     13  exposition                   9094 non-null   object 
     14  nb_rooms                     35802 non-null  float64
     15  nb_bedrooms                  34635 non-null  float64
     16  nb_bathrooms                 24095 non-null  float64
     17  nb_parking_places            37368 non-null  float64
     18  nb_boxes                     37368 non-null  float64
     19  nb_photos                    37368 non-null  float64
     20  has_a_balcony                37368 non-null  float64
     21  nb_terraces                  37368 non-null  float64
     22  has_a_cellar                 37368 non-null  float64
     23  has_a_garage                 37368 non-null  float64
     24  has_air_conditioning         37368 non-null  float64
     25  last_floor                   37368 non-null  float64
     26  upper_floors                 37368 non-null  float64
     27  price                        37368 non-null  float64
    dtypes: float64(21), int64(2), object(5)
    memory usage: 8.0+ MB


Ce code r√©alise une op√©ration de fusion (merge) entre deux DataFrames nomm√©s `features` et `labels` en utilisant une colonne commune appel√©e `id_annonce`. Voici une explication d√©taill√©e de chaque √©l√©ment du code :

1. **`df =`** :
   - Ceci indique que le r√©sultat de l'op√©ration de fusion sera assign√© √† une nouvelle variable nomm√©e `df`.

2. **`features`** :
   - Il s'agit d'un DataFrame, qui est une structure de donn√©es en pandas utilis√©e pour stocker des donn√©es tabulaires (similaire √† une table dans une base de donn√©es ou une feuille de calcul Excel). Ce DataFrame contient les caract√©ristiques (features) des annonces immobili√®res.

3. **`merge`** :
   - Il s'agit d'une m√©thode pandas utilis√©e pour combiner deux DataFrames en utilisant une cl√© commune. C'est similaire √† une jointure (JOIN) en SQL.

4. **`labels`** :
   - Il s'agit d'un autre DataFrame contenant probablement les √©tiquettes (labels) ou les valeurs cibles associ√©es aux annonces immobili√®res, comme les prix des propri√©t√©s.

5. **`on='id_annonce'`** :
   - Cela sp√©cifie la colonne commune sur laquelle la fusion doit √™tre effectu√©e. Dans ce cas, la colonne `id_annonce` est pr√©sente dans les deux DataFrames (`features` et `labels`), et elle est utilis√©e comme cl√© pour aligner les lignes correspondantes des deux DataFrames.

**Contexte et Fonctionnement :**

- **But de la Fusion :** La fusion est utilis√©e pour combiner les donn√©es des deux DataFrames de mani√®re √† ce que chaque ligne de `features` soit align√©e avec la ligne correspondante de `labels` bas√©e sur la colonne `id_annonce`.
- **R√©sultat de la Fusion :** Le DataFrame r√©sultant `df` contiendra toutes les colonnes de `features` et de `labels`, avec les lignes combin√©es l√† o√π les valeurs de `id_annonce` correspondent. Si une annonce (une valeur de `id_annonce`) est pr√©sente dans les deux DataFrames, ses caract√©ristiques et son √©tiquette seront combin√©es dans une seule ligne de `df`.

**Exemple Visuel :**

Supposons que nous ayons les deux DataFrames suivants :

`features` :
| id_annonce | size | nb_rooms |
|------------|------|----------|
| 1          | 50   | 3        |
| 2          | 75   | 4        |
| 3          | 60   | 2        |

`labels` :
| id_annonce | price |
|------------|-------|
| 1          | 250000|
| 2          | 320000|
| 3          | 210000|

Apr√®s la fusion (`merge`), nous obtenons :

`df` :
| id_annonce | size | nb_rooms | price  |
|------------|------|----------|--------|
| 1          | 50   | 3        | 250000 |
| 2          | 75   | 4        | 320000 |
| 3          | 60   | 2        | 210000 |

Ainsi, `df` contient √† la fois les caract√©ristiques et les √©tiquettes pour chaque annonce immobili√®re.

En r√©sum√©, ce code permet de combiner les donn√©es de caract√©ristiques et d'√©tiquettes en une seule table coh√©rente, facilitant ainsi les √©tapes ult√©rieures d'analyse et de mod√©lisation des donn√©es.


```python
df.head()
```


```python
df.isna().sum()
```




    id_annonce                         0
    property_type                      0
    approximate_latitude               0
    approximate_longitude              0
    city                               0
    postal_code                        0
    size                             512
    floor                          27625
    land_size                      21787
    energy_performance_value       18300
    energy_performance_category    18300
    ghg_value                      18838
    ghg_category                   18838
    exposition                     28274
    nb_rooms                        1566
    nb_bedrooms                     2733
    nb_bathrooms                   13273
    nb_parking_places                  0
    nb_boxes                           0
    nb_photos                          0
    has_a_balcony                      0
    nb_terraces                        0
    has_a_cellar                       0
    has_a_garage                       0
    has_air_conditioning               0
    last_floor                         0
    upper_floors                       0
    price                              0
    dtype: int64




```python
df.duplicated().sum()
```




    0



Le code `df.duplicated().sum()` est utilis√© pour identifier et compter le nombre de lignes dupliqu√©es dans le DataFrame `df`.

Voici une explication d√©taill√©e du fonctionnement de ce code :

1. **`df`** :
   - C'est le DataFrame sur lequel l'op√©ration est effectu√©e. Il contient les donn√©es combin√©es de caract√©ristiques et d'√©tiquettes suite √† la fusion des DataFrames `features` et `labels`.

2. **`duplicated()`** :
   - C'est une m√©thode pandas qui renvoie une s√©rie bool√©enne de la m√™me longueur que le DataFrame, o√π chaque valeur indique si la ligne correspondante est une duplication d'une ligne pr√©c√©dente. Une ligne est consid√©r√©e comme dupliqu√©e si toutes ses valeurs sont identiques √† une autre ligne pr√©c√©demment rencontr√©e dans le DataFrame.
   - Par d√©faut, `duplicated()` marque les duplications apr√®s la premi√®re occurrence (c'est-√†-dire que la premi√®re occurrence de la duplication n'est pas marqu√©e comme dupliqu√©e).

3. **`sum()`** :
   - Appliqu√© √† une s√©rie bool√©enne, `sum()` traite chaque valeur `True` comme 1 et chaque valeur `False` comme 0. En additionnant ces valeurs, `sum()` donne le nombre total de duplications (c'est-√†-dire le nombre total de `True` dans la s√©rie r√©sultante de `duplicated()`).

**Contexte et Fonctionnement :**

- **But de l'Op√©ration :** L'objectif est de d√©tecter la pr√©sence de lignes dupliqu√©es dans le DataFrame `df`, ce qui pourrait indiquer des probl√®mes dans les donn√©es, tels que des entr√©es redondantes ou des erreurs dans le processus de collecte des donn√©es.
- **R√©sultat de l'Op√©ration :** Le nombre retourn√© par `df.duplicated().sum()` est le total des lignes dupliqu√©es dans le DataFrame, ce qui aide √† √©valuer la n√©cessit√© de nettoyer ces duplications pour √©viter de biaiser les analyses ou les mod√®les de machine learning.

**Exemple Visuel :**

Supposons que nous avons le DataFrame suivant :

`df` :
| id_annonce | size | nb_rooms | price  |
|------------|------|----------|--------|
| 1          | 50   | 3        | 250000 |
| 2          | 75   | 4        | 320000 |
| 3          | 60   | 2        | 210000 |
| 2          | 75   | 4        | 320000 |  (Duplication de la 2√®me ligne)

Lorsque nous ex√©cutons `df.duplicated()`, nous obtenons :

| id_annonce | size | nb_rooms | price  | duplicated |
|------------|------|----------|--------|-------------|
| 1          | 50   | 3        | 250000 | False       |
| 2          | 75   | 4        | 320000 | False       |
| 3          | 60   | 2        | 210000 | False       |
| 2          | 75   | 4        | 320000 | True        |

Ensuite, en ex√©cutant `df.duplicated().sum()`, nous obtenons `1`, car il y a une ligne dupliqu√©e.

En r√©sum√©, `df.duplicated().sum()` est un outil pratique pour d√©tecter et compter les duplications dans un DataFrame, facilitant ainsi la gestion de la qualit√© des donn√©es avant de proc√©der √† des analyses ou des mod√®les pr√©dictifs.


```python
from sklearn.model_selection import train_test_split
seed=123
train_set, test_set = train_test_split(
    df.drop('id_annonce', axis=1), test_size=0.2,
    random_state=seed
)
print("train shape:", train_set.shape, "test shape:", test_set.shape)
```

    train shape: (29894, 27) test shape: (7474, 27)

Ce code utilise la biblioth√®que scikit-learn pour diviser un DataFrame en ensembles d'entra√Ænement et de test, ce qui est une √©tape courante dans les projets de machine learning. Voici une explication d√©taill√©e de chaque √©l√©ment du code :

1. **Importation de la Fonction :**
   ```python
   from sklearn.model_selection import train_test_split
   ```
   - Cela importe la fonction `train_test_split` depuis le module `model_selection` de scikit-learn. Cette fonction est utilis√©e pour diviser les donn√©es en ensembles d'entra√Ænement et de test de mani√®re al√©atoire.

2. **D√©finition de la graine al√©atoire :**
   ```python
   seed = 123
   ```
   - La variable `seed` est d√©finie pour stocker la valeur 123. Cette graine est utilis√©e pour assurer la reproductibilit√© des r√©sultats en fixant l'√©tat al√©atoire de la fonction de division.

3. **Division des Donn√©es :**
   ```python
   train_set, test_set = train_test_split(
       df.drop('id_annonce', axis=1), test_size=0.2,
       random_state=seed
   )
   ```
   - **`df.drop('id_annonce', axis=1)` :**
     - Cette partie du code supprime la colonne `id_annonce` du DataFrame `df` avant la division. La colonne `id_annonce` est probablement un identifiant unique qui n'est pas pertinent pour l'entra√Ænement du mod√®le.
     - `axis=1` indique que nous supprimons une colonne (et non une ligne).

   - **`train_test_split(..., test_size=0.2, random_state=seed)` :**
     - **`test_size=0.2` :** Cela signifie que 20% des donn√©es seront utilis√©es pour l'ensemble de test et 80% pour l'ensemble d'entra√Ænement.
     - **`random_state=seed` :** Cette option fixe la graine al√©atoire pour que la division soit reproductible. Chaque ex√©cution du code avec la m√™me graine donnera les m√™mes ensembles d'entra√Ænement et de test.

   - **`train_set, test_set` :** Les ensembles d'entra√Ænement et de test r√©sultants sont stock√©s dans les variables `train_set` et `test_set`, respectivement.

4. **Affichage de la Taille des Ensembles :**
   ```python
   print("train shape:", train_set.shape, "test shape:", test_set.shape)
   ```
   - Cette ligne affiche la forme (le nombre de lignes et de colonnes) des ensembles d'entra√Ænement et de test. Cela permet de v√©rifier que la division s'est faite correctement et que les proportions sp√©cifi√©es sont respect√©es.

**Contexte et Fonctionnement :**

- **But de la Division :** Diviser les donn√©es en ensembles d'entra√Ænement et de test est essentiel pour √©valuer les performances d'un mod√®le de machine learning. L'ensemble d'entra√Ænement est utilis√© pour ajuster le mod√®le, tandis que l'ensemble de test est utilis√© pour √©valuer sa performance sur des donn√©es non vues.
- **Reproductibilit√© :** Utiliser une graine al√©atoire (`random_state=seed`) permet de garantir que la division des donn√©es est la m√™me √† chaque ex√©cution, ce qui est important pour comparer les r√©sultats de diff√©rentes exp√©rimentations de mani√®re coh√©rente.

**Exemple Visuel :**

Supposons que `df` contient 1000 lignes et 10 colonnes (en incluant `id_annonce`). Apr√®s ex√©cution de ce code :

- `train_set` contiendra 800 lignes et 9 colonnes (car `id_annonce` a √©t√© supprim√©e).
- `test_set` contiendra 200 lignes et 9 colonnes.

Le r√©sultat de l'affichage sera quelque chose comme :

```
train shape: (800, 9) test shape: (200, 9)
```

En r√©sum√©, ce code divise les donn√©es en ensembles d'entra√Ænement et de test de mani√®re al√©atoire et reproductible, apr√®s avoir supprim√© une colonne non pertinente (`id_annonce`).



üñ• Analyse exploratoire


```python
real_estate = train_set.copy()
```

    - **Feature Engineering**


      Le **Feature Engineering** est le processus de cr√©ation de nouvelles caract√©ristiques (features) ou de transformation des caract√©ristiques existantes pour am√©liorer les performances des mod√®les de machine learning. Il s'agit d'une √©tape cruciale dans le pipeline de data science car les caract√©ristiques de haute qualit√© peuvent rendre les mod√®les plus pr√©cis et robustes. Voici une explication d√©taill√©e des principaux aspects du Feature Engineering :

      - Objectifs du Feature Engineering

      1. **Am√©liorer les Performances du Mod√®le :**
        - En cr√©ant ou en transformant les caract√©ristiques, on peut fournir des informations plus pertinentes et utiles au mod√®le, ce qui peut am√©liorer sa capacit√© √† apprendre et √† faire des pr√©dictions pr√©cises.

      2. **R√©duire la Dimensionalit√© :**
        - Le Feature Engineering peut inclure la s√©lection de caract√©ristiques importantes ou la combinaison de caract√©ristiques redondantes, ce qui peut r√©duire la complexit√© du mod√®le et am√©liorer ses performances.

      3. **G√©rer les Donn√©es Manquantes :**
        - Cr√©er des caract√©ristiques qui indiquent la pr√©sence de donn√©es manquantes ou utiliser des techniques pour imputer ces valeurs peut aider √† maintenir l'int√©grit√© des donn√©es.

      - Techniques Courantes de Feature Engineering

      1. **Cr√©ation de Nouvelles Caract√©ristiques :**
        - **Combinaisons de Caract√©ristiques :** Cr√©er de nouvelles caract√©ristiques en combinant les existantes, par exemple, en multipliant la taille d'une maison par le nombre de chambres pour obtenir une mesure de densit√©.
        - **Transformation des Caract√©ristiques :** Appliquer des transformations math√©matiques (logarithmique, racine carr√©e, etc.) pour modifier la distribution des donn√©es.

      2. **Encodage des Variables Cat√©gorielles :**
        - **One-Hot Encoding :** Convertir des variables cat√©gorielles en plusieurs colonnes binaires (0 ou 1).
        - **Label Encoding :** Convertir des cat√©gories en valeurs num√©riques.

      3. **Gestion des Donn√©es Manquantes :**
        - **Imputation :** Remplir les valeurs manquantes avec des moyennes, des m√©dianes, des modes ou des valeurs calcul√©es √† partir d'autres caract√©ristiques.
        - **Indicateurs de Valeurs Manquantes :** Ajouter des colonnes pour indiquer quelles valeurs sont manquantes.

      4. **Normalisation et Standardisation :**
        - **Normalisation :** Mettre les valeurs des caract√©ristiques sur une √©chelle commune (par exemple, entre 0 et 1).
        - **Standardisation :** Transformer les caract√©ristiques pour qu'elles aient une moyenne de 0 et un √©cart-type de 1.

      5. **Extraction de Caract√©ristiques :**
        - **Date et Heure :** Extraire des informations comme le jour de la semaine, le mois ou l'heure √† partir de dates.
        - **Texte :** Utiliser des techniques de traitement du langage naturel (NLP) pour extraire des caract√©ristiques des donn√©es textuelles.

      6. **R√©duction de Dimensionalit√© :**
        - Utiliser des techniques telles que l'Analyse en Composantes Principales (PCA) pour r√©duire le nombre de caract√©ristiques tout en conservant l'essentiel de l'information.

      - Exemple Pratique

      Supposons que nous ayons un ensemble de donn√©es sur des transactions immobili√®res avec les caract√©ristiques suivantes :

      - `size` (taille de la maison en m¬≤)
      - `nb_rooms` (nombre de pi√®ces)
      - `age` (√¢ge de la maison)
      - `city` (ville)

      Voici quelques exemples de Feature Engineering :

      1. **Cr√©ation d'une Nouvelle Caract√©ristique :**
        - `size_per_room` = `size` / `nb_rooms`

      2. **Encodage de la Variable Cat√©gorielle `city` :**
        - Utilisation de One-Hot Encoding pour convertir `city` en plusieurs colonnes binaires.

      3. **Imputation des Donn√©es Manquantes :**
        - Remplir les valeurs manquantes de `age` avec la moyenne des √¢ges des maisons dans les m√™mes `city`.

      4. **Transformation de la Caract√©ristique `age` :**
        - Appliquer une transformation logarithmique pour r√©duire l'effet des valeurs extr√™mes :
          - `log_age` = log(`age` + 1)

      En r√©sum√©, le Feature Engineering est une √©tape cl√© qui consiste √† cr√©er et transformer des caract√©ristiques afin de fournir des donn√©es optimis√©es et pertinentes aux mod√®les de machine learning, ce qui peut consid√©rablement am√©liorer leurs performances et leur capacit√© de g√©n√©ralisation.


```python
real_estate['living_area_per_total_land'] = real_estate['size']/real_estate['land_size']
real_estate['living_area_per_total_land']
```




    10813           NaN
    28163    262.088235
    2732       0.015584
    11636           NaN
    34955           NaN
                ...    
    7763       0.034836
    15377      0.030000
    17730      0.719424
    28030           NaN
    15725      0.226777
    Name: living_area_per_total_land, Length: 29894, dtype: float64




```python
real_estate['total_number_of_rooms'] = real_estate['nb_rooms'] + real_estate['nb_bedrooms'] + real_estate['nb_bathrooms']
real_estate['total_number_of_rooms']
```




    10813    11.0
    28163     8.0
    2732     11.0
    11636     NaN
    34955     NaN
             ... 
    7763      8.0
    15377     NaN
    17730    15.0
    28030     NaN
    15725     8.0
    Name: total_number_of_rooms, Length: 29894, dtype: float64




```python
real_estate['bedrooms_per_room'] = real_estate['nb_bedrooms']/real_estate['total_number_of_rooms']
real_estate['bedrooms_per_room']
```




    10813    0.454545
    28163    0.375000
    2732     0.272727
    11636         NaN
    34955         NaN
               ...   
    7763     0.375000
    15377         NaN
    17730    0.400000
    28030         NaN
    15725    0.375000
    Name: bedrooms_per_room, Length: 29894, dtype: float64




```python
real_estate['total_parking_capacity'] = real_estate['nb_parking_places'] + real_estate['nb_boxes']
real_estate['total_parking_capacity']
```




    10813    1.0
    28163    0.0
    2732     0.0
    11636    0.0
    34955    0.0
            ... 
    7763     0.0
    15377    1.0
    17730    0.0
    28030    1.0
    15725    1.0
    Name: total_parking_capacity, Length: 29894, dtype: float64




```python
real_estate['num_ameneties'] = real_estate['has_a_balcony'] + real_estate['has_a_cellar'] + real_estate['has_a_garage'] + real_estate['has_air_conditioning']
real_estate['num_ameneties']
```




    10813    1.0
    28163    0.0
    2732     0.0
    11636    0.0
    34955    0.0
            ... 
    7763     0.0
    15377    0.0
    17730    1.0
    28030    1.0
    15725    1.0
    Name: num_ameneties, Length: 29894, dtype: float64


Les codes ci-dessus effectuent plusieurs op√©rations de Feature Engineering sur un DataFrame appel√© `real_estate`. De nouvelles caract√©ristiques dont cr√©es √† partir des caract√©ristiques existantes afin d'am√©liorer les informations disponibles pour un √©ventuel mod√®le de machine learning. Voici une explication d√©taill√©e de chaque op√©ration :

1. **Cr√©ation de la Caract√©ristique `living_area_per_total_land` :**
   ```python
   real_estate['living_area_per_total_land'] = real_estate['size'] / real_estate['land_size']
   ```
   - **But :** Calculer la proportion de la surface habitable (`size`) par rapport √† la taille totale du terrain (`land_size`).
   - **Explication :** Cette nouvelle caract√©ristique peut donner une id√©e de l'utilisation du terrain. Une valeur √©lev√©e peut indiquer que la maison occupe une grande partie du terrain.

2. **Cr√©ation de la Caract√©ristique `total_number_of_rooms` :**
   ```python
   real_estate['total_number_of_rooms'] = real_estate['nb_rooms'] + real_estate['nb_bedrooms'] + real_estate['nb_bathrooms']
   ```
   - **But :** Calculer le nombre total de pi√®ces en additionnant les pi√®ces de vie (`nb_rooms`), les chambres (`nb_bedrooms`) et les salles de bain (`nb_bathrooms`).
   - **Explication :** Cette caract√©ristique donne une vue d'ensemble du nombre total de pi√®ces dans la maison, ce qui peut √™tre pertinent pour √©valuer la taille et la fonctionnalit√© de la propri√©t√©.

3. **Cr√©ation de la Caract√©ristique `bedrooms_per_room` :**
   ```python
   real_estate['bedrooms_per_room'] = real_estate['nb_bedrooms'] / real_estate['total_number_of_rooms']
   ```
   - **But :** Calculer la proportion de chambres par rapport au nombre total de pi√®ces.
   - **Explication :** Cette caract√©ristique peut aider √† comprendre la proportion de chambres dans la maison par rapport √† toutes les autres pi√®ces, ce qui peut √™tre un indicateur du type de propri√©t√© (par exemple, une maison plus familiale ou non).

4. **Cr√©ation de la Caract√©ristique `total_parking_capacity` :**
   ```python
   real_estate['total_parking_capacity'] = real_estate['nb_parking_places'] + real_estate['nb_boxes']
   ```
   - **But :** Calculer la capacit√© totale de stationnement en additionnant les places de parking (`nb_parking_places`) et les box/garages (`nb_boxes`).
   - **Explication :** Cette caract√©ristique donne une vue d'ensemble de la capacit√© de stationnement, ce qui peut √™tre un facteur important pour les acheteurs potentiels.

5. **Cr√©ation de la Caract√©ristique `num_ameneties` :**
   ```python
   real_estate['num_ameneties'] = real_estate['has_a_balcony'] + real_estate['has_a_cellar'] + real_estate['has_a_garage'] + real_estate['has_air_conditioning']
   ```
   - **But :** Calculer le nombre total de commodit√©s (balcon, cave, garage, climatisation) pr√©sentes dans la propri√©t√©.
   - **Explication :** Cette caract√©ristique indique combien de commodit√©s la maison offre, ce qui peut √™tre un indicateur de confort et de qualit√© de vie.

**R√©sum√© des Objectifs :**

- **`living_area_per_total_land`** : Comprendre l'utilisation du terrain par rapport √† la surface habitable.
- **`total_number_of_rooms`** : Obtenir le nombre total de pi√®ces dans la maison.
- **`bedrooms_per_room`** : √âvaluer la proportion de chambres par rapport aux autres pi√®ces.
- **`total_parking_capacity`** : Calculer la capacit√© totale de stationnement.
- **`num_ameneties`** : Quantifier le nombre de commodit√©s disponibles dans la propri√©t√©.

En cr√©ant ces nouvelles caract√©ristiques, le DataFrame `real_estate` devient plus riche en informations, ce qui peut aider les mod√®les de machine learning √† mieux comprendre les diff√©rentes facettes des propri√©t√©s et, potentiellement, √† faire des pr√©dictions plus pr√©cises sur des aspects comme le prix des biens immobiliers.


```python
from geopy.distance import geodesic

# Paris coordinates (Tour Eiffeil)
paris_latitude = 48.858370
paris_longitude = 2.294481

# Function to calculate distance between Paris and a data point
def calculate_distance(row):
    point_latitude = row['approximate_latitude']
    point_longitude = row['approximate_longitude']
    point_coordinates = (point_latitude, point_longitude)
    paris_coordinates = (paris_latitude, paris_longitude)
    distance = geodesic(point_coordinates, paris_coordinates).km
    return distance

# Apply the function to your dataframe
real_estate['distance_to_paris'] = real_estate.apply(calculate_distance, axis=1)
real_estate['distance_to_paris']
```




    10813     10.223255
    28163    438.637920
    2732     349.514431
    11636    308.683589
    34955    694.013322
                ...    
    7763     230.060205
    15377    192.529959
    17730    343.553036
    28030    305.093606
    15725    421.132323
    Name: distance_to_paris, Length: 29894, dtype: float64



Ce code utilise la biblioth√®que `geopy` pour calculer la distance g√©od√©sique (c'est-√†-dire la distance en ligne droite suivant la courbure de la Terre) entre chaque point de donn√©es dans un DataFrame et un point de r√©f√©rence, ici la Tour Eiffel √† Paris. Voici une explication d√©taill√©e de chaque partie du code :

 Importation de la Biblioth√®que

```python
from geopy.distance import geodesic
```
- Cela importe la fonction `geodesic` de la biblioth√®que `geopy`, qui est utilis√©e pour calculer la distance g√©od√©sique entre deux points sp√©cifi√©s par leurs coordonn√©es (latitude et longitude).

 D√©finition des Coordonn√©es de R√©f√©rence (Tour Eiffel)

```python
# Paris coordinates (Tour Eiffeil)
paris_latitude = 48.858370
paris_longitude = 2.294481
```
- Ces variables stockent les coordonn√©es g√©ographiques de la Tour Eiffel, qui serviront de point de r√©f√©rence pour les calculs de distance.

 D√©finition de la Fonction de Calcul de la Distance

```python
# Function to calculate distance between Paris and a data point
def calculate_distance(row):
    point_latitude = row['approximate_latitude']
    point_longitude = row['approximate_longitude']
    point_coordinates = (point_latitude, point_longitude)
    paris_coordinates = (paris_latitude, paris_longitude)
    distance = geodesic(point_coordinates, paris_coordinates).km
    return distance
```
- **`calculate_distance(row)` :** Cette fonction prend une ligne (row) du DataFrame comme argument.
  - **`point_latitude` et `point_longitude` :** Ces variables extraient les coordonn√©es de latitude et de longitude de la ligne actuelle.
  - **`point_coordinates` :** Tuple contenant les coordonn√©es de la ligne actuelle.
  - **`paris_coordinates` :** Tuple contenant les coordonn√©es de la Tour Eiffel.
  - **`distance` :** La fonction `geodesic` calcule la distance en kilom√®tres entre `point_coordinates` et `paris_coordinates`.
  - **`return distance` :** La fonction retourne la distance calcul√©e.

 Application de la Fonction au DataFrame

```python
# Apply the function to your dataframe
real_estate['distance_to_paris'] = real_estate.apply(calculate_distance, axis=1)
```
- **`real_estate.apply(calculate_distance, axis=1)` :** La m√©thode `apply` de pandas est utilis√©e pour appliquer la fonction `calculate_distance` √† chaque ligne du DataFrame `real_estate`.
  - **`axis=1` :** Indique que la fonction doit √™tre appliqu√©e sur les lignes.
- **`real_estate['distance_to_paris']` :** Une nouvelle colonne `distance_to_paris` est ajout√©e au DataFrame, contenant les distances calcul√©es pour chaque point de donn√©es par rapport √† la Tour Eiffel.

 Contexte et Utilit√©

- **But de cette Op√©ration :** Calculer la distance de chaque propri√©t√© par rapport √† un point de r√©f√©rence (la Tour Eiffel) pour √©ventuellement utiliser cette information comme caract√©ristique dans un mod√®le de machine learning. La distance √† une grande ville ou un point de rep√®re peut √™tre un indicateur important pour le prix d'un bien immobilier.
- **Utilisation Potentielle :** Cette nouvelle caract√©ristique `distance_to_paris` peut aider √† comprendre l'influence de la proximit√© √† Paris sur le prix des biens immobiliers.

 Exemple Pratique

Supposons que le DataFrame `real_estate` ait les colonnes `approximate_latitude` et `approximate_longitude` pour chaque propri√©t√© :

| approximate_latitude | approximate_longitude |
|----------------------|-----------------------|
| 48.8566              | 2.3522                |
| 48.8530              | 2.3499                |
| ...                  | ...                   |

Apr√®s application de la fonction, une nouvelle colonne `distance_to_paris` sera ajout√©e :

| approximate_latitude | approximate_longitude | distance_to_paris |
|----------------------|-----------------------|-------------------|
| 48.8566              | 2.3522                | 4.5               |
| 48.8530              | 2.3499                | 5.0               |
| ...                  | ...                   | ...               |

En r√©sum√©, ce code ajoute une nouvelle caract√©ristique `distance_to_paris` au DataFrame `real_estate`, qui contient la distance en kilom√®tres de chaque propri√©t√© √† la Tour Eiffel, apportant ainsi une information g√©ographique potentiellement utile pour l'analyse ou la mod√©lisation des donn√©es.


```python
real_estate[['city', 'distance_to_paris']].sample(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>city</th>
      <th>distance_to_paris</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5577</th>
      <td>paris-9eme</td>
      <td>3.999960</td>
    </tr>
    <tr>
      <th>10521</th>
      <td>gien</td>
      <td>132.357926</td>
    </tr>
    <tr>
      <th>18339</th>
      <td>l'hay-les-roses</td>
      <td>8.212950</td>
    </tr>
    <tr>
      <th>26581</th>
      <td>montreuil</td>
      <td>10.707766</td>
    </tr>
    <tr>
      <th>5741</th>
      <td>lacaune</td>
      <td>573.011402</td>
    </tr>
  </tbody>
</table>
</div>




```python
# R√©sum√© statistique
real_estate.describe().T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>approximate_latitude</th>
      <td>29894.0</td>
      <td>46.554705</td>
      <td>2.354314</td>
      <td>41.374436</td>
      <td>43.930923</td>
      <td>46.965949</td>
      <td>48.842526</td>
      <td>5.104601e+01</td>
    </tr>
    <tr>
      <th>approximate_longitude</th>
      <td>29894.0</td>
      <td>2.607762</td>
      <td>2.592422</td>
      <td>-4.733545</td>
      <td>1.089791</td>
      <td>2.378397</td>
      <td>4.567152</td>
      <td>9.483665e+00</td>
    </tr>
    <tr>
      <th>postal_code</th>
      <td>29894.0</td>
      <td>53712.291898</td>
      <td>28786.435880</td>
      <td>1000.000000</td>
      <td>30210.000000</td>
      <td>59000.000000</td>
      <td>78230.000000</td>
      <td>9.588000e+04</td>
    </tr>
    <tr>
      <th>size</th>
      <td>29477.0</td>
      <td>1095.707060</td>
      <td>5629.776972</td>
      <td>1.000000</td>
      <td>73.000000</td>
      <td>115.000000</td>
      <td>239.000000</td>
      <td>4.113110e+05</td>
    </tr>
    <tr>
      <th>floor</th>
      <td>7813.0</td>
      <td>3.457827</td>
      <td>6.628996</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>5.500000e+01</td>
    </tr>
    <tr>
      <th>land_size</th>
      <td>12424.0</td>
      <td>4005.178686</td>
      <td>59071.859008</td>
      <td>1.000000</td>
      <td>363.000000</td>
      <td>797.000000</td>
      <td>1836.250000</td>
      <td>6.203700e+06</td>
    </tr>
    <tr>
      <th>energy_performance_value</th>
      <td>15210.0</td>
      <td>206.996121</td>
      <td>872.853538</td>
      <td>0.000000</td>
      <td>125.000000</td>
      <td>180.000000</td>
      <td>240.000000</td>
      <td>1.000000e+05</td>
    </tr>
    <tr>
      <th>ghg_value</th>
      <td>14791.0</td>
      <td>32.323372</td>
      <td>322.041271</td>
      <td>0.000000</td>
      <td>8.000000</td>
      <td>16.000000</td>
      <td>36.000000</td>
      <td>1.702400e+04</td>
    </tr>
    <tr>
      <th>nb_rooms</th>
      <td>28634.0</td>
      <td>4.234826</td>
      <td>2.935642</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>1.250000e+02</td>
    </tr>
    <tr>
      <th>nb_bedrooms</th>
      <td>27707.0</td>
      <td>2.854946</td>
      <td>2.147069</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>1.180000e+02</td>
    </tr>
    <tr>
      <th>nb_bathrooms</th>
      <td>19277.0</td>
      <td>0.919956</td>
      <td>0.271941</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>3.000000e+00</td>
    </tr>
    <tr>
      <th>nb_parking_places</th>
      <td>29894.0</td>
      <td>0.293504</td>
      <td>0.455375</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>nb_boxes</th>
      <td>29894.0</td>
      <td>0.178999</td>
      <td>0.383358</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>nb_photos</th>
      <td>29894.0</td>
      <td>7.972637</td>
      <td>4.649109</td>
      <td>1.000000</td>
      <td>5.000000</td>
      <td>8.000000</td>
      <td>10.000000</td>
      <td>5.000000e+01</td>
    </tr>
    <tr>
      <th>has_a_balcony</th>
      <td>29894.0</td>
      <td>0.149194</td>
      <td>0.356285</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>nb_terraces</th>
      <td>29894.0</td>
      <td>0.305914</td>
      <td>0.460801</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>has_a_cellar</th>
      <td>29894.0</td>
      <td>0.201177</td>
      <td>0.400887</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>has_a_garage</th>
      <td>29894.0</td>
      <td>0.053857</td>
      <td>0.225739</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>has_air_conditioning</th>
      <td>29894.0</td>
      <td>0.039306</td>
      <td>0.194324</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>last_floor</th>
      <td>29894.0</td>
      <td>0.003680</td>
      <td>0.060550</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>upper_floors</th>
      <td>29894.0</td>
      <td>0.000268</td>
      <td>0.016357</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>price</th>
      <td>29894.0</td>
      <td>341586.581388</td>
      <td>308223.958671</td>
      <td>24465.000000</td>
      <td>155000.000000</td>
      <td>255000.000000</td>
      <td>412375.000000</td>
      <td>2.299000e+06</td>
    </tr>
    <tr>
      <th>living_area_per_total_land</th>
      <td>12420.0</td>
      <td>3.566579</td>
      <td>68.298408</td>
      <td>0.000015</td>
      <td>0.090015</td>
      <td>0.196907</td>
      <td>0.490631</td>
      <td>5.039000e+03</td>
    </tr>
    <tr>
      <th>total_number_of_rooms</th>
      <td>18736.0</td>
      <td>8.011422</td>
      <td>4.857124</td>
      <td>0.000000</td>
      <td>6.000000</td>
      <td>8.000000</td>
      <td>10.000000</td>
      <td>2.370000e+02</td>
    </tr>
    <tr>
      <th>bedrooms_per_room</th>
      <td>17208.0</td>
      <td>0.335426</td>
      <td>0.057611</td>
      <td>0.000000</td>
      <td>0.300000</td>
      <td>0.333333</td>
      <td>0.375000</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <th>total_parking_capacity</th>
      <td>29894.0</td>
      <td>0.472503</td>
      <td>0.598309</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000e+00</td>
    </tr>
    <tr>
      <th>num_ameneties</th>
      <td>29894.0</td>
      <td>0.443534</td>
      <td>0.658244</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>3.000000e+00</td>
    </tr>
    <tr>
      <th>distance_to_paris</th>
      <td>29894.0</td>
      <td>336.492172</td>
      <td>244.442169</td>
      <td>0.449444</td>
      <td>86.167824</td>
      <td>362.702396</td>
      <td>575.148604</td>
      <td>9.907175e+02</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Table de fr√©quence des variables qualitatives
for col in list(real_estate.select_dtypes(include='object')):
  print('-----------Colonne :', col, 'avec ', real_estate[col].nunique(), ' modalit√©s-------------')
  print(real_estate[col].value_counts(normalize=True))
  print('\n')
```

    -----------Colonne : property_type avec  22  modalit√©s-------------
    property_type
    appartement          0.421757
    maison               0.419315
    divers               0.057403
    terrain              0.041614
    villa                0.020272
    propri√©t√©            0.010470
    terrain √† b√¢tir      0.007794
    duplex               0.005921
    viager               0.004616
    ferme                0.003512
    parking              0.002710
    loft                 0.001338
    chalet               0.001338
    ch√¢teau              0.000602
    moulin               0.000368
    manoir               0.000335
    p√©niche              0.000201
    h√¥tel particulier    0.000167
    chambre              0.000134
    g√Æte                 0.000067
    h√¥tel                0.000033
    atelier              0.000033
    Name: proportion, dtype: float64
    
    
    -----------Colonne : city avec  7702  modalit√©s-------------
    city
    toulouse                0.012544
    montpellier             0.010035
    paris-16eme             0.006590
    paris-17eme             0.006322
    paris-18eme             0.006055
                              ...   
    villelaure              0.000033
    la-forest-landerneau    0.000033
    ners                    0.000033
    briquenay               0.000033
    saulgond                0.000033
    Name: proportion, Length: 7702, dtype: float64
    
    
    -----------Colonne : energy_performance_category avec  7  modalit√©s-------------
    energy_performance_category
    D    0.366601
    C    0.215713
    E    0.201118
    B    0.112755
    F    0.054635
    A    0.032479
    G    0.016700
    Name: proportion, dtype: float64
    
    
    -----------Colonne : ghg_category avec  7  modalit√©s-------------
    ghg_category
    B    0.242648
    C    0.184910
    D    0.176594
    E    0.160773
    A    0.137313
    F    0.069705
    G    0.028058
    Name: proportion, dtype: float64
    
    
    -----------Colonne : exposition avec  12  modalit√©s-------------
    exposition
    Sud           0.355760
    Sud-Ouest     0.149252
    Sud-Est       0.104902
    Est-Ouest     0.090897
    Ouest         0.090485
    Est           0.076617
    Sud-Nord      0.038446
    Nord-Ouest    0.037073
    Nord          0.025127
    Nord-Est      0.024715
    Ouest-Est     0.003845
    Nord-Sud      0.002883
    Name: proportion, dtype: float64
    
    

Ce code cr√©e et affiche des tableaux de fr√©quence pour chaque variable qualitative (ou cat√©gorielle) dans le DataFrame `real_estate`. Voici une explication d√©taill√©e de chaque partie du code :

 S√©lection des Colonnes Qualitatives

```python
real_estate.select_dtypes(include='object')
```
- **`real_estate.select_dtypes(include='object')` :** Cette m√©thode de pandas s√©lectionne toutes les colonnes du DataFrame `real_estate` qui ont un type de donn√©es 'object'. En pandas, les colonnes de type 'object' sont g√©n√©ralement utilis√©es pour repr√©senter des variables qualitatives ou cat√©gorielles.

 Boucle sur les Colonnes Qualitatives

```python
for col in list(real_estate.select_dtypes(include='object')):
```
- **`for col in list(...)` :** La boucle `for` it√®re sur la liste des noms des colonnes qualitatives s√©lectionn√©es.
- **`col` :** √Ä chaque it√©ration, `col` repr√©sente le nom d'une colonne qualitative dans le DataFrame `real_estate`.

 Affichage des Informations pour Chaque Colonne

   Impression de l'En-t√™te de la Colonne

  ```python
  print('-----------Colonne :', col, 'avec ', real_estate[col].nunique(), ' modalit√©s-------------')
  ```
  - **`print(...)` :** Affiche le nom de la colonne actuelle (`col`), ainsi que le nombre de modalit√©s (ou valeurs distinctes) qu'elle contient.
  - **`real_estate[col].nunique()` :** Retourne le nombre de valeurs uniques dans la colonne `col`.

   Affichage des Fr√©quences des Modalit√©s

  ```python
  print(real_estate[col].value_counts(normalize=True))
  ```
  - **`real_estate[col].value_counts(normalize=True)` :** Cette m√©thode compte le nombre d'occurrences de chaque valeur dans la colonne `col` et normalise les r√©sultats pour obtenir des proportions (fr√©quences relatives) au lieu de simples comptages.
  - **`normalize=True` :** Indique que les fr√©quences relatives (proportions) doivent √™tre calcul√©es.

   Ajout d'une Ligne Blanche pour la Lisibilit√©

  ```python
  print('\n')
  ```
  - **`print('\n')` :** Ajoute une ligne blanche apr√®s l'affichage des fr√©quences pour am√©liorer la lisibilit√© des r√©sultats.

 Ex√©cution du Code

Pour illustrer avec un exemple pratique, supposons que le DataFrame `real_estate` contient deux colonnes qualitatives : `city` et `property_type`.

1. **S√©lection des colonnes qualitatives :**
   ```python
   real_estate.select_dtypes(include='object')
   ```
   Imaginons que cela retourne les colonnes `['city', 'property_type']`.

2. **Boucle sur les colonnes :**
   ```python
   for col in list(real_estate.select_dtypes(include='object')):
   ```

3. **Premi√®re it√©ration (`col = 'city'`) :**
   - **Affichage de l'en-t√™te :**
     ```python
     print('-----------Colonne :', col, 'avec ', real_estate[col].nunique(), ' modalit√©s-------------')
     ```
     Supposons que `real_estate['city'].nunique()` retourne `3`.

     **Sortie :**
     ```
     -----------Colonne : city avec  3  modalit√©s-------------
     ```

   - **Affichage des fr√©quences :**
     ```python
     print(real_estate[col].value_counts(normalize=True))
     ```
     Supposons que `real_estate['city'].value_counts(normalize=True)` retourne :
     ```
     Paris     0.5
     Lyon      0.3
     Marseille 0.2
     ```

     **Sortie :**
     ```
     Paris        0.5
     Lyon         0.3
     Marseille    0.2
     ```

   - **Ligne blanche :**
     ```python
     print('\n')
     ```

4. **Deuxi√®me it√©ration (`col = 'property_type'`) :**
   - **Affichage de l'en-t√™te :**
     ```python
     print('-----------Colonne :', col, 'avec ', real_estate[col].nunique(), ' modalit√©s-------------')
     ```
     Supposons que `real_estate['property_type'].nunique()` retourne `2`.

     **Sortie :**
     ```
     -----------Colonne : property_type avec  2  modalit√©s-------------
     ```

   - **Affichage des fr√©quences :**
     ```python
     print(real_estate[col].value_counts(normalize=True))
     ```
     Supposons que `real_estate['property_type'].value_counts(normalize=True)` retourne :
     ```
     Apartment    0.7
     House        0.3
     ```

     **Sortie :**
     ```
     Apartment    0.7
     House        0.3
     ```

   - **Ligne blanche :**
     ```python
     print('\n')
     ```

En r√©sum√©, ce code affiche pour chaque variable qualitative dans le DataFrame `real_estate` le nombre de modalit√©s et les fr√©quences relatives de chaque modalit√©, ce qui est utile pour comprendre la distribution des valeurs cat√©gorielles et identifier les cat√©gories dominantes.



```python
# Histogrammes
real_estate.hist(bins=50, figsize=(18, 13));
```


    
![png](french_real_estate_prediction_files/french_real_estate_prediction_22_0.png)
    



```python
import seaborn as sns
import matplotlib.pyplot as plt
sns.scatterplot(x='approximate_longitude', y='approximate_latitude', data=real_estate)
plt.xlabel('approximate_longitude')
plt.ylabel('approximate_latitude')
plt.title('Scatter plot of real estate data')
plt.show()

```


    
![png](french_real_estate_prediction_files/french_real_estate_prediction_23_0.png)
    



```python
sns.scatterplot(x='approximate_longitude', 
                y='approximate_latitude', 
                size='price', 
                hue='price', 
                data=real_estate)
plt.xlabel('approximate_longitude')
plt.ylabel('approximate_latitude')
plt.title('Scatter plot of real estate data')
plt.legend(title='Price')
plt.show()
```


    
![png](french_real_estate_prediction_files/french_real_estate_prediction_24_0.png)
    



```python
plt.figure(figsize=(10, 6)) 
sns.scatterplot(x='approximate_longitude', y='approximate_latitude', 
                size='price', hue='property_type', 
                data=real_estate, palette='tab10')
plt.xlabel('approximate_longitude')
plt.ylabel('approximate_latitude')
plt.title('Scatter plot of real estate data')
plt.legend(title='Property Type')
plt.show()

```


    
![png](french_real_estate_prediction_files/french_real_estate_prediction_25_0.png)
    



```python

filtered_data = real_estate[(real_estate['property_type'] == 'appartement') | (real_estate['property_type'] == 'maison')]
plt.figure(figsize=(10, 6))  # D√©finition de la taille de la figure
sns.scatterplot(x='approximate_longitude', y='approximate_latitude', 
                size='price', hue='property_type', 
                data=filtered_data, palette='tab10')
plt.xlabel('approximate_longitude')
plt.ylabel('approximate_latitude')
plt.title('Scatter plot of real estate data (Apartments and Houses)')
plt.legend(title='Property Type')
plt.show()
```


Ce code effectue les op√©rations suivantes pour cr√©er un graphique de dispersion (scatter plot) qui visualise la distribution g√©ographique des biens immobiliers, en mettant en √©vidence la diff√©rence entre les appartements et les maisons. Voici une explication d√©taill√©e de chaque √©tape :

 Filtrage des Donn√©es

```python
filtered_data = real_estate[(real_estate['property_type'] == 'appartement') | (real_estate['property_type'] == 'maison')]
```
- **`real_estate[...]`** : Cette expression s√©lectionne les lignes du DataFrame `real_estate` o√π la colonne `property_type` est soit "appartement" soit "maison".
- **`filtered_data`** : Le r√©sultat de cette s√©lection est stock√© dans une nouvelle variable `filtered_data`, qui contient uniquement les biens immobiliers de type appartement et maison.

 Configuration de la Taille de la Figure

```python
plt.figure(figsize=(10, 6))  # D√©finition de la taille de la figure
```
- **`plt.figure(figsize=(10, 6))`** : Cette fonction de Matplotlib d√©finit la taille de la figure du graphique, ici de 10 pouces de large et 6 pouces de haut.

 Cr√©ation du Scatter Plot

```python
sns.scatterplot(x='approximate_longitude', y='approximate_latitude', 
                size='price', hue='property_type', 
                data=filtered_data, palette='tab10')
```
- **`sns.scatterplot(...)`** : Cette fonction de Seaborn cr√©e un graphique de dispersion.
  - **`x='approximate_longitude'`** : Les coordonn√©es longitudinales des propri√©t√©s sont utilis√©es pour l'axe des x.
  - **`y='approximate_latitude'`** : Les coordonn√©es latitudinales des propri√©t√©s sont utilis√©es pour l'axe des y.
  - **`size='price'`** : La taille des points sur le graphique est proportionnelle au prix des propri√©t√©s.
  - **`hue='property_type'`** : Les points sont color√©s selon le type de propri√©t√© (appartement ou maison).
  - **`data=filtered_data`** : Le DataFrame filtr√© est utilis√© comme source de donn√©es.
  - **`palette='tab10'`** : Une palette de couleurs sp√©cifique est utilis√©e pour diff√©rencier les types de propri√©t√©s.

 Ajout des √âtiquettes et du Titre

```python
plt.xlabel('approximate_longitude')
plt.ylabel('approximate_latitude')
plt.title('Scatter plot of real estate data (Apartments and Houses)')
```
- **`plt.xlabel('approximate_longitude')`** : D√©finit l'√©tiquette de l'axe des x comme "approximate_longitude".
- **`plt.ylabel('approximate_latitude')`** : D√©finit l'√©tiquette de l'axe des y comme "approximate_latitude".
- **`plt.title('Scatter plot of real estate data (Apartments and Houses)')`** : D√©finit le titre du graphique.

 Affichage de la L√©gende

```python
plt.legend(title='Property Type')
```
- **`plt.legend(title='Property Type')`** : Ajoute une l√©gende au graphique avec le titre "Property Type" pour expliquer les couleurs correspondant aux types de propri√©t√©s.

 Affichage du Graphique

```python
plt.show()
```
- **`plt.show()`** : Affiche le graphique.

 Contexte et Utilit√©

- **But du Graphique :** Visualiser la r√©partition g√©ographique des appartements et des maisons, avec une indication visuelle des prix des propri√©t√©s.
- **Utilisation Potentielle :** Ce graphique peut aider √† identifier des tendances g√©ographiques, comme des concentrations de propri√©t√©s de certains types ou des zones o√π les prix sont plus √©lev√©s.

En r√©sum√©, ce code cr√©e un graphique de dispersion pour visualiser la localisation g√©ographique des biens immobiliers en France, en mettant en √©vidence les diff√©rences entre les appartements et les maisons et en repr√©sentant le prix des propri√©t√©s par la taille des points.


```python
import numpy as np
plt.figure(figsize=(18,10))

# Exclure les colonnes non num√©riques
numeric_real_estate = real_estate.select_dtypes(include=[np.number])

real_estate_corr = numeric_real_estate.corr()
mask = np.triu(np.ones_like(real_estate_corr, dtype=bool))
sns.heatmap(
    real_estate_corr, mask=mask, center=0,
    cmap='RdBu', linewidths=1, annot=True,
    fmt=".2f", vmin=-1, vmax=1
)
plt.title("Carte des corr√©lations")
plt.show()
```

Ce code utilise la biblioth√®que NumPy et la biblioth√®que de visualisation Matplotlib avec son module pyplot pour cr√©er une carte de chaleur (heatmap) des corr√©lations entre les diff√©rentes variables num√©riques d'un ensemble de donn√©es sur l'immobilier.

Voici une explication ligne par ligne :

1. `import numpy as np`: Importe la biblioth√®que NumPy sous l'alias np.
2. `import matplotlib.pyplot as plt`: Importe la biblioth√®que Matplotlib avec son module pyplot sous l'alias plt.
3. `plt.figure(figsize=(18,10))`: Cr√©e une nouvelle figure avec une taille de 18 pouces de largeur et 10 pouces de hauteur.
4. `numeric_real_estate = real_estate.select_dtypes(include=[np.number])`: S√©lectionne uniquement les colonnes num√©riques de l'ensemble de donn√©es `real_estate` (qui est suppos√© √™tre d√©fini ailleurs dans le code mais n'est pas pr√©sent dans l'extrait que vous avez fourni).
5. `real_estate_corr = numeric_real_estate.corr()`: Calcule la matrice des corr√©lations entre les variables num√©riques de l'ensemble de donn√©es.
6. `mask = np.triu(np.ones_like(real_estate_corr, dtype=bool))`: Cr√©e un masque pour masquer la moiti√© sup√©rieure de la carte de chaleur, car la matrice des corr√©lations est sym√©trique par rapport √† sa diagonale.
7. `sns.heatmap(...)` : Trace une carte de chaleur des corr√©lations en utilisant la fonction heatmap de la biblioth√®que Seaborn (qui n'a pas √©t√© import√©e dans le code fourni mais qui est n√©cessaire pour cette ligne). Les arguments sp√©cifi√©s incluent la matrice de corr√©lation, le masque pour la moiti√© sup√©rieure, le centre de la colormap, la colormap utilis√©e (RdBu), l'√©paisseur des lignes, l'annotation des valeurs sur la carte, le format des annotations, les valeurs minimales et maximales pour la colormap.
8. `plt.title("Carte des corr√©lations")`: D√©finit le titre de la carte de chaleur.
9. `plt.show()`: Affiche la carte de chaleur.

Ce code est utile pour visualiser les relations lin√©aires entre les variables num√©riques d'un ensemble de donn√©es et peut aider √† identifier des mod√®les ou des tendances.
    

```python
for col in list(real_estate.select_dtypes(include='object').drop('city', axis=1)):
  print("-----------------price VS", col, "------------------------")
  print(real_estate.groupby(col)['price'].mean().sort_values())
  sns.boxplot(data=real_estate, x='price', y=col)
  plt.show()
  plt.close()
```

Ce code parcourt les colonnes de type "objet" (c'est-√†-dire les colonnes cat√©goriques) d'un ensemble de donn√©es `real_estate` √† l'exception de la colonne 'city'. Pour chaque colonne, il affiche les statistiques de prix (en moyenne) par cat√©gorie de cette colonne et trace √©galement une bo√Æte √† moustaches (boxplot) pour visualiser la distribution des prix par cat√©gorie.

Voici une explication ligne par ligne :

1. `for col in list(real_estate.select_dtypes(include='object').drop('city', axis=1)):`: Parcourt chaque colonne de l'ensemble de donn√©es `real_estate` qui est de type "objet" (c'est-√†-dire cat√©gorique), √† l'exception de la colonne 'city'.
   
2. `print("-----------------price VS", col, "------------------------")`: Affiche une ligne de s√©paration et le nom de la colonne actuellement analys√©e.

3. `print(real_estate.groupby(col)['price'].mean().sort_values())`: Calcule la moyenne des prix (`price`) pour chaque cat√©gorie de la colonne actuellement analys√©e (`col`) et trie les r√©sultats par ordre croissant. Cela affiche les moyennes des prix pour chaque cat√©gorie de la colonne.

4. `sns.boxplot(data=real_estate, x='price', y=col)`: Trace un boxplot avec les prix (`price`) en tant que variable ind√©pendante et la colonne cat√©gorique actuellement analys√©e (`col`) en tant que variable d√©pendante.

5. `plt.show()`: Affiche le boxplot.

6. `plt.close()`: Ferme la figure apr√®s l'avoir affich√©e. Cela est n√©cessaire car sans cette instruction, la boucle continuerait √† tracer sur la m√™me figure, superposant les graphiques pr√©c√©dents.

Ce code est utile pour visualiser la relation entre les variables cat√©goriques et les prix dans un ensemble de donn√©es, ce qui peut aider √† identifier des tendances ou des diff√©rences significatives dans les prix en fonction des cat√©gories.



    -----------------price VS property_type ------------------------
    property_type
    parking              5.801333e+04
    chambre              1.063750e+05
    terrain √† b√¢tir      1.083508e+05
    terrain              1.218439e+05
    viager               1.785017e+05
    divers               3.059696e+05
    ferme                3.214845e+05
    maison               3.255814e+05
    appartement          3.720782e+05
    chalet               3.986384e+05
    duplex               4.086984e+05
    moulin               4.706545e+05
    h√¥tel                4.910000e+05
    loft                 5.049762e+05
    villa                5.257610e+05
    g√Æte                 5.550000e+05
    propri√©t√©            6.504257e+05
    p√©niche              6.925000e+05
    h√¥tel particulier    7.083800e+05
    manoir               7.822050e+05
    ch√¢teau              1.078403e+06
    atelier              1.300000e+06
    Name: price, dtype: float64



    
![png](french_real_estate_prediction_files/french_real_estate_prediction_28_1.png)
    


    -----------------price VS energy_performance_category ------------------------
    energy_performance_category
    G    216562.129921
    F    246950.354994
    E    303041.066362
    D    352285.848458
    A    384611.190283
    C    434601.926242
    B    439656.787755
    Name: price, dtype: float64



    
![png](french_real_estate_prediction_files/french_real_estate_prediction_28_3.png)
    


    -----------------price VS ghg_category ------------------------
    ghg_category
    G    279869.831325
    F    339485.526673
    C    355154.803291
    B    355794.675676
    E    377837.122372
    A    379216.728705
    D    387317.205590
    Name: price, dtype: float64



    
![png](french_real_estate_prediction_files/french_real_estate_prediction_28_5.png)
    


    -----------------price VS exposition ------------------------
    exposition
    Nord          306179.398907
    Nord-Ouest    321697.211111
    Sud-Nord      358697.864286
    Ouest         366705.218513
    Est           367180.910394
    Sud           392291.823234
    Sud-Est       402456.744764
    Est-Ouest     405793.345921
    Nord-Est      435204.138889
    Sud-Ouest     441905.737810
    Nord-Sud      496062.142857
    Ouest-Est     512836.821429
    Name: price, dtype: float64



    
![png](french_real_estate_prediction_files/french_real_estate_prediction_28_7.png)
    



üñ• **Data preprocessing : Pr√©traitement des donn√©es**


```python
trainset = train_set.copy()
```


```python
X_train = trainset.drop("price", axis=1)
y_train = trainset['price']
```

    - Pipeline de pr√©traitement des variables num√©riques


```python
# variables num√©riques
vars_num = list(trainset.drop('price', axis=1).select_dtypes(exclude='object'))
vars_num
```




    ['approximate_latitude',
     'approximate_longitude',
     'postal_code',
     'size',
     'floor',
     'land_size',
     'energy_performance_value',
     'ghg_value',
     'nb_rooms',
     'nb_bedrooms',
     'nb_bathrooms',
     'nb_parking_places',
     'nb_boxes',
     'nb_photos',
     'has_a_balcony',
     'nb_terraces',
     'has_a_cellar',
     'has_a_garage',
     'has_air_conditioning',
     'last_floor',
     'upper_floors']




```python
# Fonction d'ajout de nouvelles variables
def add_features(Z):
  Z['living_area_per_total_land'] = Z['size']/Z['land_size']
  Z['total_number_of_rooms'] = Z['nb_rooms'] + Z['nb_bedrooms'] + Z['nb_bathrooms']
  Z['bedrooms_per_room'] = Z['nb_bedrooms']/Z['total_number_of_rooms']
  Z['total_parking_capacity'] = Z['nb_parking_places'] + Z['nb_boxes']
  Z['num_ameneties'] = Z['has_a_balcony'] + Z['has_a_cellar'] + Z['has_a_garage'] + Z['has_air_conditioning']
  Z['distance_to_paris'] = Z.apply(calculate_distance, axis=1)
  return Z.values

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
num_pipeline = Pipeline([
    ('feats_adder', FunctionTransformer(add_features)),
    ('num_impute', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

num_pipeline
```


Ce code d√©finit un pipeline de transformation pour pr√©traiter les caract√©ristiques num√©riques d'un ensemble de donn√©es. Voici une explication ligne par ligne :

1. **D√©finition de la fonction d'ajout de nouvelles variables** (`add_features`):
   - Cette fonction prend un DataFrame `Z` en entr√©e et ajoute plusieurs nouvelles variables bas√©es sur les caract√©ristiques existantes du DataFrame.
   - Les nouvelles variables ajout√©es sont :
     - `living_area_per_total_land` : la surface habitable par rapport √† la taille totale du terrain.
     - `total_number_of_rooms` : le nombre total de pi√®ces, qui est la somme des chambres, des salles de bains et des salons.
     - `bedrooms_per_room` : le ratio de chambres par rapport au nombre total de pi√®ces.
     - `total_parking_capacity` : la capacit√© totale de stationnement, qui est la somme des places de parking et des garages.
     - `num_ameneties` : le nombre total d'√©quipements disponibles, tels que le balcon, la cave, le garage et la climatisation.
     - `distance_to_paris` : la distance √† Paris, calcul√©e √† partir d'une fonction `calculate_distance` appliqu√©e √† chaque ligne du DataFrame.
   - La fonction retourne les valeurs du DataFrame transform√©.

2. **Importation des modules n√©cessaires** :
   - `from sklearn.pipeline import Pipeline`: Importe la classe `Pipeline` de scikit-learn pour cr√©er un pipeline de transformation.
   - `from sklearn.preprocessing import FunctionTransformer, StandardScaler`: Importe des transformateurs de scikit-learn pour appliquer des transformations personnalis√©es et standardiser les caract√©ristiques num√©riques.
   - `from sklearn.impute import SimpleImputer`: Importe la classe `SimpleImputer` pour g√©rer les valeurs manquantes dans les donn√©es.

3. **Cr√©ation du pipeline** (`num_pipeline`):
   - `Pipeline([...])`: Cr√©e un pipeline de transformation pour les caract√©ristiques num√©riques.
   - Les √©tapes du pipeline sont :
     - `('feats_adder', FunctionTransformer(add_features))`: Applique la fonction `add_features` pour ajouter de nouvelles variables aux donn√©es.
     - `('num_impute', SimpleImputer(strategy='median'))`: Impute les valeurs manquantes en utilisant la m√©diane des valeurs existantes.
     - `('scaler', StandardScaler())`: Standardise les caract√©ristiques en soustrayant la moyenne et en divisant par l'√©cart-type pour centrer les donn√©es autour de z√©ro et mettre √† l'√©chelle pour avoir une variance unitaire.

4. **Affichage du pipeline** :
   - `num_pipeline`: Affiche le pipeline cr√©√©.

Ce pipeline peut √™tre utilis√© pour pr√©traiter les caract√©ristiques num√©riques d'un ensemble de donn√©es, en ajoutant de nouvelles variables, en imputant les valeurs manquantes et en standardisant les donn√©es.


    - Pipeline de pr√©traitement des variables cat√©gorielles


```python
# Variables cat√©gorielles
vars_cat = list(trainset.select_dtypes(include='object').drop('city', axis=1))
vars_cat
```




    ['property_type', 'energy_performance_category', 'ghg_category', 'exposition']




```python
cat_pipeline = Pipeline([
    ('cat_imputer', SimpleImputer(strategy='constant', fill_value="UNKNOWN")),
    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist', min_frequency=0.002))
])
cat_pipeline
```

Ce code cr√©e un pipeline de transformation pour pr√©traiter les caract√©ristiques cat√©goriques d'un ensemble de donn√©es. Voici une explication ligne par ligne :

1. **S√©lection des caract√©ristiques cat√©goriques** :
   - `vars_cat = list(trainset.select_dtypes(include='object').drop('city', axis=1))`: S√©lectionne les noms des colonnes cat√©goriques de l'ensemble de donn√©es `trainset`, √† l'exception de la colonne 'city'.

2. **Cr√©ation du pipeline de transformation** (`cat_pipeline`) :
   - `Pipeline([...])`: Cr√©e un pipeline de transformation pour les caract√©ristiques cat√©goriques.
   - Les √©tapes du pipeline sont :
     - `('cat_imputer', SimpleImputer(strategy='constant', fill_value="UNKNOWN"))`: Utilise l'imputation constante pour remplacer les valeurs manquantes par la cha√Æne de caract√®res "UNKNOWN". Cela signifie que toute valeur manquante dans les caract√©ristiques cat√©goriques sera remplac√©e par "UNKNOWN".
     - `('encoder', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist', min_frequency=0.002))`: Utilise un encodeur one-hot pour transformer les caract√©ristiques cat√©goriques en variables binaires. Les arguments sp√©cifi√©s sont :
        - `sparse_output=False` : pour obtenir une sortie dense plut√¥t que sparse.
        - `handle_unknown='infrequent_if_exist'` : sp√©cifie comment g√©rer les cat√©gories inconnues lors de la transformation. Si une cat√©gorie inconnue est rencontr√©e pendant la transformation, elle sera trait√©e comme une cat√©gorie rare.
        - `min_frequency=0.002` : sp√©cifie la fr√©quence minimale d'apparition d'une cat√©gorie pour qu'elle soit consid√©r√©e comme fr√©quente. Les cat√©gories dont la fr√©quence est inf√©rieure √† cette valeur seront trait√©es comme des cat√©gories rares.

Ce pipeline peut √™tre utilis√© pour pr√©traiter les caract√©ristiques cat√©goriques d'un ensemble de donn√©es en rempla√ßant les valeurs manquantes par "UNKNOWN" et en les encodant en variables binaires √† l'aide d'un encodage one-hot.


    - Combinaison des deux pipelines en un seul pipeline


```python
from sklearn.compose import ColumnTransformer
preprocessor = ColumnTransformer([
    ("num", num_pipeline, vars_num),
    ("cat", cat_pipeline, vars_cat)
])
preprocessor
```

Ce code utilise la classe `ColumnTransformer` de scikit-learn pour cr√©er un pr√©processeur qui applique des transformations sp√©cifiques aux colonnes num√©riques et cat√©goriques d'un ensemble de donn√©es. Voici une explication ligne par ligne :

1. **Importation du module n√©cessaire** :
   - `from sklearn.compose import ColumnTransformer`: Importe la classe `ColumnTransformer` de scikit-learn, qui permet de transformer diff√©rentes colonnes de l'ensemble de donn√©es de mani√®re sp√©cifique.

2. **Cr√©ation du pr√©processeur** (`preprocessor`) :
   - `preprocessor = ColumnTransformer([...])`: Cr√©e un pr√©processeur qui applique des transformations sp√©cifiques aux diff√©rentes colonnes de l'ensemble de donn√©es.
   - Les √©tapes du pr√©processeur sont d√©finies dans une liste, o√π chaque √©l√©ment est un tuple contenant trois √©l√©ments :
     - Le premier √©l√©ment du tuple est une cha√Æne de caract√®res qui identifie la transformation.
     - Le deuxi√®me √©l√©ment est le transformateur √† appliquer sur les colonnes s√©lectionn√©es.
     - Le troisi√®me √©l√©ment est une liste des noms des colonnes sur lesquelles appliquer la transformation.
   
3. **Transformation des colonnes num√©riques** :
   - `("num", num_pipeline, vars_num)`: Applique le pipeline de transformation `num_pipeline` (d√©fini pr√©c√©demment) sur les colonnes num√©riques. `vars_num` est une liste des noms des colonnes num√©riques s√©lectionn√©es.
   
4. **Transformation des colonnes cat√©goriques** :
   - `("cat", cat_pipeline, vars_cat)`: Applique le pipeline de transformation `cat_pipeline` (√©galement d√©fini pr√©c√©demment) sur les colonnes cat√©goriques. `vars_cat` est une liste des noms des colonnes cat√©goriques s√©lectionn√©es.

En combinant ces transformations dans un seul `ColumnTransformer`, le pr√©processeur peut √™tre utilis√© pour appliquer toutes les √©tapes de pr√©traitement n√©cessaires sur les colonnes num√©riques et cat√©goriques de l'ensemble de donn√©es en une seule op√©ration.


```python
# Application aux donn√©es d'entrainement
X_train_prepared = preprocessor.fit_transform(X_train)
print(X_train_prepared.shape)
X_train_prepared
```

    (29894, 67)





    array([[ 0.98030347, -0.06715911,  1.36829606, ...,  0.        ,
             1.        ,  0.        ],
           [-0.39282477, -1.40638835, -1.26840651, ...,  0.        ,
             1.        ,  0.        ],
           [-0.24320179, -0.84242234, -1.2979348 , ...,  0.        ,
             1.        ,  0.        ],
           ...,
           [-0.2605051 , -0.70053739, -1.29550306, ...,  0.        ,
             1.        ,  0.        ],
           [ 0.90957735, -1.71950283, -0.6410172 , ...,  0.        ,
             1.        ,  0.        ],
           [-0.09496148,  1.48887805,  0.71172586, ...,  0.        ,
             1.        ,  0.        ]])



    - Exp√©rimentations ML pour la s√©lection de mod√®le


```python
# R√©gression lin√©aire
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_train_prepared, y_train)
```




<style>#sk-container-id-4 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-4 {
  color: var(--sklearn-color-text);
}

#sk-container-id-4 pre {
  padding: 0;
}

#sk-container-id-4 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-4 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-4 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-4 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-4 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-4 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-4 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-4 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-4 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-4 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-4 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-4 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "‚ñ∏";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-4 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "‚ñæ";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-4 div.sk-label label.sk-toggleable__label,
#sk-container-id-4 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-4 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-4 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-4 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-4 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-4 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-4 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-4 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-4 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-16" type="checkbox" checked><label for="sk-estimator-id-16" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LinearRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LinearRegression()</pre></div> </div></div></div></div>


Ce code accomplit plusieurs t√¢ches en utilisant un mod√®le de r√©gression lin√©aire pour effectuer une pr√©diction :

1. **Transformation des donn√©es d'entra√Ænement** :
   - `X_train_prepared = preprocessor.fit_transform(X_train)`: Les donn√©es d'entra√Ænement `X_train` sont pr√©trait√©es en utilisant le pr√©processeur `preprocessor`. Cela implique l'application des transformations sp√©cifiques d√©finies pour les colonnes num√©riques et cat√©goriques. Les colonnes num√©riques sont transform√©es √† l'aide du pipeline `num_pipeline`, tandis que les colonnes cat√©goriques sont transform√©es √† l'aide du pipeline `cat_pipeline`. Cette op√©ration pr√©pare les donn√©es pour √™tre utilis√©es dans le mod√®le de r√©gression lin√©aire.

2. **Entra√Ænement du mod√®le de r√©gression lin√©aire** :
   - `from sklearn.linear_model import LinearRegression`: Importe la classe `LinearRegression` de scikit-learn pour cr√©er un mod√®le de r√©gression lin√©aire.
   - `lin_reg = LinearRegression()`: Initialise un objet de mod√®le de r√©gression lin√©aire.
   - `lin_reg.fit(X_train_prepared, y_train)`: Entra√Æne le mod√®le de r√©gression lin√©aire en utilisant les donn√©es pr√©trait√©es `X_train_prepared` en tant que variables explicatives et les √©tiquettes `y_train` correspondantes comme valeurs cibles. Le mod√®le ajuste les coefficients de r√©gression pour minimiser l'erreur quadratique moyenne entre les pr√©dictions et les valeurs r√©elles.

En r√©sum√©, ce code pr√©pare les donn√©es d'entra√Ænement en les pr√©traitant √† l'aide d'un `preprocessor` qui applique des transformations sp√©cifiques aux colonnes num√©riques et cat√©goriques, puis entra√Æne un mod√®le de r√©gression lin√©aire √† l'aide de ces donn√©es pr√©trait√©es.



```python
# Pr√©dictions
y_train_preds = lin_reg.predict(X_train_prepared)

# RMSE
from sklearn.metrics import root_mean_squared_error
lin_rmse = root_mean_squared_error(y_train, y_train_preds)
lin_rmse
```




    268519.6895255891


Ce code effectue une pr√©diction sur les donn√©es d'entra√Ænement en utilisant le mod√®le de r√©gression lin√©aire entra√Æn√© pr√©c√©demment, puis calcule la racine de l'erreur quadratique moyenne (RMSE) entre les pr√©dictions et les valeurs cibles r√©elles. Voici une explication ligne par ligne :

1. **Pr√©diction sur les donn√©es d'entra√Ænement** :
   - `y_train_preds = lin_reg.predict(X_train_prepared)`: Le mod√®le de r√©gression lin√©aire (`lin_reg`) est utilis√© pour pr√©dire les valeurs cibles (`y_train_preds`) en utilisant les donn√©es d'entra√Ænement pr√©trait√©es (`X_train_prepared`). Cela donne les pr√©dictions du mod√®le sur les donn√©es d'entra√Ænement.

2. **Importation de la m√©trique de performance** :
   - `from sklearn.metrics import root_mean_squared_error`: Importe la fonction `root_mean_squared_error` de scikit-learn, qui calcule la racine de l'erreur quadratique moyenne (RMSE). Cette m√©trique est utilis√©e pour √©valuer les performances du mod√®le de r√©gression.

3. **Calcul de la RMSE** :
   - `lin_rmse = root_mean_squared_error(y_train, y_train_preds)`: Calcule la RMSE entre les valeurs cibles r√©elles (`y_train`) et les pr√©dictions du mod√®le (`y_train_preds`). La RMSE mesure l'√©cart moyen entre les pr√©dictions du mod√®le et les valeurs r√©elles, en tenant compte de la variance des erreurs. Cette valeur est stock√©e dans la variable `lin_rmse`.

En r√©sum√©, ce code √©value les performances du mod√®le de r√©gression lin√©aire sur les donn√©es d'entra√Ænement en calculant la RMSE entre les pr√©dictions du mod√®le et les valeurs cibles r√©elles.


```python
# Validation crois√©e
from sklearn.model_selection import cross_val_score
scores = cross_val_score(lin_reg, X_train_prepared, y_train, 
                         scoring="neg_mean_squared_error", cv=3)
lin_rmse_scores = np.sqrt(-scores)
lin_rmse_scores
```




    array([272673.19731209, 272029.12417556, 269864.04921785])




```python
def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())
```


```python
display_scores(lin_rmse_scores)
```

    Scores: [272673.19731209 272029.12417556 269864.04921785]
    Mean: 271522.1235685009
    Standard deviation: 1201.5588390367857


Ce code utilise la validation crois√©e pour √©valuer les performances du mod√®le de r√©gression lin√©aire sur les donn√©es d'entra√Ænement. Voici une explication ligne par ligne :

1. **Importation du module n√©cessaire** :
   - `from sklearn.model_selection import cross_val_score`: Importe la fonction `cross_val_score` de scikit-learn, qui permet de r√©aliser la validation crois√©e.

2. **√âvaluation du mod√®le par validation crois√©e** :
   - `scores = cross_val_score(lin_reg, X_train_prepared, y_train, scoring="neg_mean_squared_error", cv=3)`: Utilise la fonction `cross_val_score` pour √©valuer le mod√®le de r√©gression lin√©aire (`lin_reg`) en utilisant une validation crois√©e √† 3 plis (`cv=3`). Les donn√©es d'entra√Ænement pr√©trait√©es (`X_train_prepared`) et les valeurs cibles (`y_train`) sont utilis√©es. La m√©trique de performance utilis√©e est l'oppos√© de l'erreur quadratique moyenne (`scoring="neg_mean_squared_error"`), car `cross_val_score` maximise les valeurs de score, mais dans ce cas, nous voulons minimiser l'erreur quadratique moyenne.

3. **Conversion des scores en RMSE** :
   - `lin_rmse_scores = np.sqrt(-scores)`: Prend l'oppos√© des scores de l'erreur quadratique moyenne (qui ont √©t√© n√©gativ√©s pour des raisons de convention dans scikit-learn) et calcule la racine carr√©e pour obtenir les scores de la racine de l'erreur quadratique moyenne (RMSE). Ces valeurs sont stock√©es dans `lin_rmse_scores`.

4. **D√©finition d'une fonction pour afficher les scores** :
   - `def display_scores(scores)`: D√©finit une fonction `display_scores` qui prend en entr√©e les scores et affiche les scores, la moyenne des scores et l'√©cart type des scores.
   - `print("Scores:", scores)`: Affiche les scores de la validation crois√©e.
   - `print("Mean:", scores.mean())`: Affiche la moyenne des scores.
   - `print("Standard deviation:", scores.std())`: Affiche l'√©cart type des scores.

5. **Affichage des scores de validation crois√©e et de leurs statistiques** :
   - `display_scores(lin_rmse_scores)`: Appelle la fonction `display_scores` pour afficher les scores de la RMSE et leurs statistiques.

En r√©sum√©, ce code √©value les performances du mod√®le de r√©gression lin√©aire sur les donn√©es d'entra√Ænement en utilisant la validation crois√©e et affiche les scores de la RMSE ainsi que leurs statistiques.


En machine learning, la validation crois√©e est une technique essentielle pour √©valuer la performance des mod√®les pr√©dictifs et estimer leur capacit√© √† g√©n√©raliser √† de nouvelles donn√©es non vues. Elle consiste √† diviser l'ensemble de donn√©es en sous-ensembles plus petits appel√©s "plis", puis √† entra√Æner et √©valuer le mod√®le plusieurs fois en utilisant diff√©rents plis comme ensemble de validation, tandis que les autres plis sont utilis√©s comme ensemble d'entra√Ænement.

La validation crois√©e est importante pour plusieurs raisons :

1. **Estimation fiable de la performance du mod√®le** : La validation crois√©e fournit une estimation plus fiable de la performance du mod√®le que la simple division de l'ensemble de donn√©es en ensembles d'entra√Ænement et de test. En moyennant les performances sur plusieurs it√©rations de validation, elle r√©duit le risque de biais li√© √† la s√©lection al√©atoire des ensembles d'entra√Ænement et de test.

2. **Utilisation efficace des donn√©es** : En utilisant chaque observation √† la fois comme donn√©es d'entra√Ænement et de test √† diff√©rentes it√©rations, la validation crois√©e permet de maximiser l'utilisation des donn√©es disponibles pour l'entra√Ænement et l'√©valuation du mod√®le.

3. **√âvaluation de la capacit√© de g√©n√©ralisation** : La validation crois√©e fournit une estimation de la capacit√© du mod√®le √† g√©n√©raliser √† de nouvelles donn√©es non vues. En testant le mod√®le sur des ensembles de donn√©es diff√©rents de ceux utilis√©s pour l'entra√Ænement, elle √©value sa capacit√© √† faire des pr√©dictions pr√©cises sur des donn√©es inconnues.

4. **D√©tection du surajustement (overfitting)** : La validation crois√©e permet de d√©tecter le surajustement en fournissant une estimation plus fiable des performances du mod√®le sur des donn√©es r√©elles. Si le mod√®le pr√©sente des performances √©lev√©es sur les ensembles d'entra√Ænement mais des performances m√©diocres sur les ensembles de validation, cela peut indiquer un surajustement.

En r√©sum√©, la validation crois√©e est une technique essentielle en machine learning pour √©valuer la performance des mod√®les, estimer leur capacit√© √† g√©n√©raliser √† de nouvelles donn√©es et d√©tecter le surajustement. Elle fournit une √©valuation plus fiable et robuste des mod√®les pr√©dictifs, ce qui est crucial pour la prise de d√©cision dans de nombreuses applications.



```python
def train_and_evaluate(ml_algo):
  reg = ml_algo
  reg.fit(X_train_prepared, y_train)

  y_train_predictions = reg.predict(X_train_prepared)
  reg_rmse = root_mean_squared_error(y_train, y_train_predictions)
  print("RMSE:", reg_rmse)

  reg_scores = cross_val_score(reg, X_train_prepared, y_train, 
                               scoring="neg_mean_squared_error", cv=3)
  reg_rmse_scores = np.sqrt(-reg_scores)
  display_scores(reg_rmse_scores)
  return reg
```


```python
all_models = []
from sklearn.ensemble import RandomForestRegressor
from xgboost.sklearn import XGBRegressor
from sklearn.neural_network import MLPRegressor
for ml_algo in [RandomForestRegressor(random_state=seed), 
                XGBRegressor(random_state=seed)]:
  print(ml_algo)
  model = train_and_evaluate(ml_algo)
  all_models.append(model)
  print('\n')
```

    RandomForestRegressor(random_state=123)
    RMSE: 56579.313618180415
    Scores: [158875.65614356 157124.34426933 158029.27379189]
    Mean: 158009.7580682598
    Standard deviation: 715.1032409389485
    
    
    XGBRegressor(base_score=None, booster=None, callbacks=None,
                 colsample_bylevel=None, colsample_bynode=None,
                 colsample_bytree=None, device=None, early_stopping_rounds=None,
                 enable_categorical=False, eval_metric=None, feature_types=None,
                 gamma=None, grow_policy=None, importance_type=None,
                 interaction_constraints=None, learning_rate=None, max_bin=None,
                 max_cat_threshold=None, max_cat_to_onehot=None,
                 max_delta_step=None, max_depth=None, max_leaves=None,
                 min_child_weight=None, missing=nan, monotone_constraints=None,
                 multi_strategy=None, n_estimators=None, n_jobs=None,
                 num_parallel_tree=None, random_state=123, ...)
    RMSE: 102855.8063586502
    Scores: [155600.20801531 153316.95177794 153926.69687876]
    Mean: 154281.28555733606
    Standard deviation: 965.268394318689
    
    
Ce code d√©finit une fonction `train_and_evaluate` qui prend un algorithme d'apprentissage automatique en entr√©e, entra√Æne ce mod√®le sur les donn√©es d'entra√Ænement, √©value sa performance et retourne le mod√®le entra√Æn√©. Ensuite, il boucle sur une liste d'algorithmes d'apprentissage automatique (Random Forest et XGBoost) et utilise cette fonction pour entra√Æner et √©valuer chaque mod√®le.

Explications d√©taill√©es du code :

1. **D√©finition de la fonction `train_and_evaluate`** :
   - Cette fonction prend un algorithme d'apprentissage automatique (`ml_algo`) en entr√©e.
   - Elle entra√Æne ce mod√®le sur les donn√©es d'entra√Ænement pr√©par√©es (`X_train_prepared`, `y_train`).
   - Elle effectue des pr√©dictions sur les donn√©es d'entra√Ænement pour calculer la racine de l'erreur quadratique moyenne (RMSE) entre les valeurs cibles r√©elles et les pr√©dictions du mod√®le.
   - Elle utilise la validation crois√©e √† 3 plis pour √©valuer la performance du mod√®le en utilisant l'oppos√© de l'erreur quadratique moyenne comme m√©trique de performance.
   - Elle affiche la RMSE sur les donn√©es d'entra√Ænement et les scores de RMSE moyens et √©cart-type obtenus par la validation crois√©e.
   - Enfin, elle retourne le mod√®le entra√Æn√©.

2. **Initialisation de la liste `all_models`** :
   - `all_models = []` : Initialise une liste vide `all_models` qui stockera tous les mod√®les entra√Æn√©s.

3. **Importation des modules n√©cessaires** :
   - `from sklearn.ensemble import RandomForestRegressor` : Importe la classe `RandomForestRegressor` de scikit-learn pour entra√Æner un mod√®le de for√™t al√©atoire.
   - `from xgboost.sklearn import XGBRegressor` : Importe la classe `XGBRegressor` de la biblioth√®que XGBoost pour entra√Æner un mod√®le de boosting de gradient.
   - `from sklearn.neural_network import MLPRegressor` : Importe la classe `MLPRegressor` de scikit-learn pour entra√Æner un mod√®le de r√©seau de neurones.

4. **Boucle sur les algorithmes d'apprentissage automatique** :
   - La boucle it√®re sur une liste contenant des instances d'algorithmes d'apprentissage automatique : `RandomForestRegressor` et `XGBRegressor`.
   - Pour chaque algorithme, il affiche l'algorithme utilis√©, puis appelle la fonction `train_and_evaluate` pour entra√Æner et √©valuer le mod√®le.
   - Le mod√®le entra√Æn√© est ensuite ajout√© √† la liste `all_models`.

En r√©sum√©, ce code entra√Æne et √©value plusieurs mod√®les d'apprentissage automatique (Random Forest et XGBoost) en utilisant une fonction commune pour entra√Æner et √©valuer chaque mod√®le. Il fournit √©galement des informations sur la performance de chaque mod√®le sur les donn√©es d'entra√Ænement et via la validation crois√©e.


- R√©glage des hyperparam√®tres


```python
num_pipeline = Pipeline([
    ('feats_adder', FunctionTransformer(add_features)),
    ('num_impute', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_pipeline = Pipeline([
    ('cat_imputer', SimpleImputer(strategy='constant', fill_value="UNKNOWN")),
    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist', min_frequency=0.002))
])

preprocessor = ColumnTransformer([
    ("num", num_pipeline, vars_num),
    ("cat", cat_pipeline, vars_cat)
])

full_pipeline = Pipeline([
    ("preprocess", preprocessor),
    ('model', XGBRegressor(random_state=seed))
])

full_pipeline
```


```python
full_pipeline[1]
```

Ce code d√©finit plusieurs pipelines pour pr√©traiter les caract√©ristiques num√©riques et cat√©goriques, puis les fusionne dans un pipeline complet qui pr√©traite les donn√©es et entra√Æne un mod√®le de r√©gression XGBoost. Voici une explication ligne par ligne :

1. **Pipeline pour les caract√©ristiques num√©riques (`num_pipeline`)** :
   - Ce pipeline contient trois √©tapes :
     - `('feats_adder', FunctionTransformer(add_features))`: Utilise la fonction `add_features` pour ajouter de nouvelles variables aux caract√©ristiques num√©riques.
     - `('num_impute', SimpleImputer(strategy='median'))`: Impute les valeurs manquantes en utilisant la m√©diane des valeurs existantes.
     - `('scaler', StandardScaler())`: Standardise les caract√©ristiques en centrant les donn√©es autour de z√©ro et en les mettant √† l'√©chelle pour avoir une variance unitaire.

2. **Pipeline pour les caract√©ristiques cat√©goriques (`cat_pipeline`)** :
   - Ce pipeline contient deux √©tapes :
     - `('cat_imputer', SimpleImputer(strategy='constant', fill_value="UNKNOWN"))`: Impute les valeurs manquantes en rempla√ßant par "UNKNOWN".
     - `('encoder', OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist', min_frequency=0.002))`: Encode les caract√©ristiques cat√©goriques en utilisant un encodage one-hot, avec des options pour g√©rer les cat√©gories inconnues et minimiser la fr√©quence des cat√©gories rares.

3. **ColumnTransformer (`preprocessor`)** :
   - Utilise le `num_pipeline` pour pr√©traiter les caract√©ristiques num√©riques (`vars_num`) et le `cat_pipeline` pour pr√©traiter les caract√©ristiques cat√©goriques (`vars_cat`).

4. **Pipeline complet (`full_pipeline`)** :
   - Ce pipeline contient deux √©tapes :
     - `("preprocess", preprocessor)`: Utilise le `preprocessor` pour pr√©traiter les caract√©ristiques num√©riques et cat√©goriques.
     - `('model', XGBRegressor(random_state=seed))`: Utilise un mod√®le de r√©gression XGBoost comme mod√®le final pour l'apprentissage.
  
5. **Affichage du pipeline complet (`full_pipeline`)** :
   - `full_pipeline`: Affiche le pipeline complet, montrant toutes les √©tapes de pr√©traitement et de mod√©lisation.

6. **Acc√®s au mod√®le entra√Æn√© dans le pipeline complet** :
   - `full_pipeline[1]`: Permet d'acc√©der au deuxi√®me √©l√©ment du pipeline complet, qui est le mod√®le de r√©gression XGBoost. Cela peut √™tre utile pour acc√©der au mod√®le entra√Æn√© et utiliser ses m√©thodes ou attributs apr√®s l'entra√Ænement.

En r√©sum√©, ce code d√©finit un pipeline complet qui pr√©traite les caract√©ristiques num√©riques et cat√©goriques, puis entra√Æne un mod√®le de r√©gression XGBoost. Ce pipeline peut √™tre utilis√© pour entra√Æner et √©valuer le mod√®le de mani√®re coh√©rente et r√©p√©table.



```python
X_train = train_set.drop('price', axis=1)
y_train = train_set['price']
```


```python
from scipy.stats import randint

param_dist = {
    'model__n_estimators': randint(low=150, high=200),
    'model__max_depth': randint(low=5, high=10),
    'model__learning_rate': np.arange(0.05, 1, 0.05),
    'model__colsample_bytree': np.arange(0.5, 1, 0.05)
}

from sklearn.model_selection import RandomizedSearchCV
rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_dist,
    n_iter=15, cv=3, scoring='neg_mean_squared_error',
    random_state=seed, n_jobs=-1
)

rnd_search.fit(X_train, y_train)

print("Meilleurs hyperparam√®tres :", rnd_search.best_params_)
print("Meilleur score :", np.sqrt(-rnd_search.best_score_))
```

    Meilleurs hyperparam√®tres : {'model__colsample_bytree': 0.9000000000000004, 'model__learning_rate': 0.2, 'model__max_depth': 7, 'model__n_estimators': 155}
    Meilleur score : 150460.43190867038



Ce code effectue une recherche al√©atoire des hyperparam√®tres pour un mod√®le XGBoost en utilisant la validation crois√©e pour √©valuer la performance des combinaisons d'hyperparam√®tres. Voici une explication ligne par ligne :

1. **Pr√©paration des donn√©es d'entra√Ænement** :
   - `X_train = train_set.drop('price', axis=1)`: S√©pare les caract√©ristiques des √©tiquettes en enlevant la colonne 'price' des donn√©es d'entra√Ænement.
   - `y_train = train_set['price']`: Extrait la colonne 'price' comme √©tiquettes des donn√©es d'entra√Ænement.

2. **D√©finition de l'espace des hyperparam√®tres** (`param_dist`) :
   - `param_dist`: Un dictionnaire d√©finissant les distributions de probabilit√© des hyperparam√®tres √† explorer dans la recherche al√©atoire. Les hyperparam√®tres sp√©cifi√©s sont :
     - `'model__n_estimators'`: Le nombre d'estimateurs dans le mod√®le XGBoost.
     - `'model__max_depth'`: La profondeur maximale de chaque arbre dans le mod√®le XGBoost.
     - `'model__learning_rate'`: Le taux d'apprentissage du mod√®le XGBoost.
     - `'model__colsample_bytree'`: La proportion de caract√©ristiques √† consid√©rer pour chaque arbre dans le mod√®le XGBoost.

3. **Importation de la classe `RandomizedSearchCV`** :
   - `from sklearn.model_selection import RandomizedSearchCV`: Importe la classe `RandomizedSearchCV` de scikit-learn, qui effectue une recherche al√©atoire des hyperparam√®tres avec validation crois√©e.

4. **Initialisation de l'objet `RandomizedSearchCV`** (`rnd_search`) :
   - `rnd_search = RandomizedSearchCV(...)` : Initialise un objet `RandomizedSearchCV` avec les param√®tres suivants :
     - `estimator=full_pipeline`: Le pipeline complet contenant le mod√®le XGBoost et les pr√©traitements.
     - `param_distributions=param_dist`: Les distributions d'hyperparam√®tres √† explorer.
     - `n_iter=15`: Le nombre d'it√©rations de la recherche al√©atoire.
     - `cv=3`: Le nombre de plis pour la validation crois√©e.
     - `scoring='neg_mean_squared_error'`: La m√©trique de performance √† optimiser, dans ce cas, l'oppos√© de l'erreur quadratique moyenne.
     - `random_state=seed`: La graine al√©atoire pour la reproductibilit√©.
     - `n_jobs=-1`: Utilise tous les c≈ìurs disponibles pour acc√©l√©rer le processus.

5. **Ex√©cution de la recherche al√©atoire** :
   - `rnd_search.fit(X_train, y_train)`: Ex√©cute la recherche al√©atoire en ajustant le mod√®le √† l'ensemble de donn√©es d'entra√Ænement.

6. **Affichage des meilleurs hyperparam√®tres et du meilleur score** :
   - `print("Meilleurs hyperparam√®tres :", rnd_search.best_params_)`: Affiche les meilleurs hyperparam√®tres trouv√©s par la recherche al√©atoire.
   - `print("Meilleur score :", np.sqrt(-rnd_search.best_score_))`: Affiche le meilleur score de performance, qui est la racine de l'oppos√© de l'erreur quadratique moyenne, √©valu√© sur l'ensemble de validation crois√©e.

En r√©sum√©, ce code effectue une recherche al√©atoire des hyperparam√®tres pour un mod√®le XGBoost en utilisant la validation crois√©e pour √©valuer la performance des diff√©rentes combinaisons d'hyperparam√®tres. Il s√©lectionne les meilleurs hyperparam√®tres et affiche le meilleur score de performance obtenu.


En machine learning, un hyperparam√®tre est un param√®tre dont la valeur est fix√©e avant le d√©but du processus d'apprentissage. Contrairement aux param√®tres du mod√®le, qui sont appris √† partir des donn√©es d'entra√Ænement, les hyperparam√®tres ne sont pas directement appris par le mod√®le, mais plut√¥t sp√©cifi√©s par le praticien de l'apprentissage automatique avant le processus d'entra√Ænement.

Voici quelques exemples courants d'hyperparam√®tres dans diff√©rents algorithmes d'apprentissage automatique :

1. **Dans les mod√®les de r√©gression lin√©aire** :
   - L'hyperparam√®tre pourrait √™tre le terme de r√©gularisation (par exemple, le coefficient de p√©nalit√© dans la r√©gression de Ridge ou Lasso).

2. **Dans les mod√®les d'arbre de d√©cision** :
   - L'hyperparam√®tre pourrait √™tre la profondeur maximale de l'arbre, le nombre minimal d'observations dans les feuilles, ou le nombre minimum d'observations n√©cessaires pour diviser un n≈ìud.

3. **Dans les mod√®les de r√©seaux de neurones** :
   - Les hyperparam√®tres pourraient inclure le nombre de couches, le nombre de neurones dans chaque couche, le taux d'apprentissage, etc.

Le r√©glage des hyperparam√®tres, √©galement appel√© optimisation des hyperparam√®tres, fait r√©f√©rence au processus de s√©lection des valeurs optimales pour les hyperparam√®tres d'un mod√®le afin d'optimiser ses performances sur un ensemble de donn√©es donn√©. C'est un aspect crucial de la construction de mod√®les performants en machine learning.

Le r√©glage des hyperparam√®tres peut se faire de mani√®re exhaustive en essayant diff√©rentes combinaisons d'hyperparam√®tres √† l'aide d'une validation crois√©e pour √©valuer la performance de chaque combinaison. Il peut √©galement se faire de mani√®re plus efficace √† l'aide de techniques d'optimisation automatique telles que la recherche al√©atoire, la recherche par grille, ou des algorithmes d'optimisation plus sophistiqu√©s comme la recherche bay√©sienne.

En r√©sum√©, les hyperparam√®tres sont des param√®tres d√©finis avant l'entra√Ænement du mod√®le, et le r√©glage des hyperparam√®tres consiste √† s√©lectionner les valeurs optimales de ces param√®tres pour maximiser les performances du mod√®le sur les donn√©es d'entra√Ænement et de test.



```python
pd.DataFrame(rnd_search.cv_results_)
```


```python
final_model = rnd_search.best_estimator_
final_model
```



Ce code utilise les r√©sultats de la recherche al√©atoire des hyperparam√®tres (`rnd_search`) pour cr√©er un DataFrame Pandas contenant ces r√©sultats, puis s√©lectionne le meilleur mod√®le trouv√© par la recherche al√©atoire.

1. **Cr√©ation d'un DataFrame √† partir des r√©sultats de la recherche al√©atoire** :
   - `pd.DataFrame(rnd_search.cv_results_)`: Utilise la fonction `pd.DataFrame()` pour cr√©er un DataFrame Pandas √† partir des r√©sultats de la recherche al√©atoire (`rnd_search.cv_results_`). Ces r√©sultats contiennent des informations sur les diff√©rents param√®tres test√©s, les scores de validation crois√©e et d'autres m√©triques associ√©es.

2. **S√©lection du meilleur mod√®le** :
   - `final_model = rnd_search.best_estimator_`: S√©lectionne le meilleur mod√®le trouv√© par la recherche al√©atoire en acc√©dant √† l'attribut `best_estimator_` de l'objet `rnd_search`. Cet attribut contient l'estimateur final (c'est-√†-dire le mod√®le) qui a obtenu le meilleur score de performance lors de la recherche al√©atoire.

En r√©sum√©, ce code extrait les r√©sultats de la recherche al√©atoire des hyperparam√®tres dans un DataFrame Pandas pour une analyse plus approfondie, puis s√©lectionne le meilleur mod√®le trouv√© par la recherche al√©atoire pour une utilisation ult√©rieure. Cela permet d'acc√©der facilement aux performances et aux d√©tails des diff√©rents mod√®les test√©s lors de la recherche des hyperparam√®tres, ainsi qu'au mod√®le optimal pour la pr√©diction.


```python
# Donn√©es de test
X_test = test_set.drop("price", axis=1)
y_test = test_set['price']
```


```python
# Pr√©dictions sur les donn√©es de test
final_preds = final_model.predict(X_test)
final_rmse = root_mean_squared_error(y_test, final_preds)
final_rmse
```




    145401.679231076




```python
# Importance des attributs
feature_importances = final_model[1].feature_importances_
feature_importances
```




    array([0.01415569, 0.01989408, 0.01661087, 0.0342943 , 0.00701508,
           0.0168217 , 0.01055061, 0.00716285, 0.1057459 , 0.09275771,
           0.0027499 , 0.00859006, 0.00503853, 0.00901752, 0.00492183,
           0.02368234, 0.00634518, 0.00993551, 0.02150142, 0.00709649,
           0.        , 0.01088047, 0.0185196 , 0.00741489, 0.00682468,
           0.00577252, 0.11137699, 0.05187562, 0.03596661, 0.00904498,
           0.0088913 , 0.01877839, 0.01491495, 0.03699737, 0.01593864,
           0.00605421, 0.02725793, 0.01397329, 0.019726  , 0.00120924,
           0.00424008, 0.00791157, 0.01056543, 0.00286114, 0.00053156,
           0.00232237, 0.01415903, 0.00394749, 0.00546943, 0.00478915,
           0.006143  , 0.00392316, 0.01119221, 0.        , 0.01214399,
           0.00237037, 0.00952335, 0.00128693, 0.00520556, 0.00428585,
           0.00442623, 0.00599695, 0.0123602 , 0.00719747, 0.0065831 ,
           0.00525913, 0.        ], dtype=float32)




```python
len(feature_importances)
```




    67




```python
extra_attribs = [
    'living_area_per_total_land',
    'total_number_of_rooms',
    'bedrooms_per_room',
    'total_parking_capacity',
    'num_ameneties',
    'distance_to_paris'
]

cat_ohe_attribs = list(final_model[0].named_transformers_['cat'].get_feature_names_out())

attributes = vars_num + extra_attribs + cat_ohe_attribs
print(len(attributes))
attributes
```

    67





    ['approximate_latitude',
     'approximate_longitude',
     'postal_code',
     'size',
     'floor',
     'land_size',
     'energy_performance_value',
     'ghg_value',
     'nb_rooms',
     'nb_bedrooms',
     'nb_bathrooms',
     'nb_parking_places',
     'nb_boxes',
     'nb_photos',
     'has_a_balcony',
     'nb_terraces',
     'has_a_cellar',
     'has_a_garage',
     'has_air_conditioning',
     'last_floor',
     'upper_floors',
     'living_area_per_total_land',
     'total_number_of_rooms',
     'bedrooms_per_room',
     'total_parking_capacity',
     'num_ameneties',
     'distance_to_paris',
     'property_type_appartement',
     'property_type_divers',
     'property_type_duplex',
     'property_type_ferme',
     'property_type_maison',
     'property_type_parking',
     'property_type_propri√©t√©',
     'property_type_terrain',
     'property_type_terrain √† b√¢tir',
     'property_type_viager',
     'property_type_villa',
     'property_type_infrequent_sklearn',
     'energy_performance_category_A',
     'energy_performance_category_B',
     'energy_performance_category_C',
     'energy_performance_category_D',
     'energy_performance_category_E',
     'energy_performance_category_F',
     'energy_performance_category_G',
     'energy_performance_category_UNKNOWN',
     'ghg_category_A',
     'ghg_category_B',
     'ghg_category_C',
     'ghg_category_D',
     'ghg_category_E',
     'ghg_category_F',
     'ghg_category_G',
     'ghg_category_UNKNOWN',
     'exposition_Est',
     'exposition_Est-Ouest',
     'exposition_Nord',
     'exposition_Nord-Est',
     'exposition_Nord-Ouest',
     'exposition_Ouest',
     'exposition_Sud',
     'exposition_Sud-Est',
     'exposition_Sud-Nord',
     'exposition_Sud-Ouest',
     'exposition_UNKNOWN',
     'exposition_infrequent_sklearn']




```python
importance_result = sorted(zip(feature_importances, attributes), reverse=True)
imp_df = pd.DataFrame(importance_result, 
                      columns=["Score d'importance", "Variable"])
imp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Score d'importance</th>
      <th>Variable</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.111377</td>
      <td>distance_to_paris</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.105746</td>
      <td>nb_rooms</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.092758</td>
      <td>nb_bedrooms</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.051876</td>
      <td>property_type_appartement</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.036997</td>
      <td>property_type_propri√©t√©</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>62</th>
      <td>0.001209</td>
      <td>energy_performance_category_A</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.000532</td>
      <td>energy_performance_category_F</td>
    </tr>
    <tr>
      <th>64</th>
      <td>0.000000</td>
      <td>upper_floors</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.000000</td>
      <td>ghg_category_G</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.000000</td>
      <td>exposition_infrequent_sklearn</td>
    </tr>
  </tbody>
</table>
<p>67 rows √ó 2 columns</p>
</div>


Ce code effectue plusieurs √©tapes pour √©valuer les performances du mod√®le final sur l'ensemble de test et pour analyser l'importance des variables dans les pr√©dictions du mod√®le. Voici une explication ligne par ligne :

1. **Pr√©paration des donn√©es de test** :
   - `X_test = test_set.drop("price", axis=1)`: S√©pare les caract√©ristiques des √©tiquettes en enlevant la colonne 'price' des donn√©es de test.
   - `y_test = test_set['price']`: Extrait la colonne 'price' comme √©tiquettes des donn√©es de test.

2. **Pr√©diction sur les donn√©es de test** :
   - `final_preds = final_model.predict(X_test)`: Utilise le mod√®le final (`final_model`) pour faire des pr√©dictions sur les donn√©es de test.

3. **Calcul de la RMSE sur les donn√©es de test** :
   - `final_rmse = root_mean_squared_error(y_test, final_preds)`: Calcule la racine de l'erreur quadratique moyenne (RMSE) entre les valeurs cibles r√©elles et les pr√©dictions du mod√®le sur les donn√©es de test.

4. **Extraction de l'importance des variables** :
   - `feature_importances = final_model[1].feature_importances_`: Extrait l'importance des variables √† partir du mod√®le final. Pour le mod√®le XGBoost, cette information est g√©n√©ralement disponible dans l'attribut `feature_importances_`.

5. **Cr√©ation de la liste des variables suppl√©mentaires et des variables cat√©gorielles encod√©es en one-hot** :
   - `extra_attribs`: Liste des variables suppl√©mentaires ajout√©es lors du pr√©traitement.
   - `cat_ohe_attribs = list(final_model[0].named_transformers_['cat'].get_feature_names_out())`: Obtient les noms des caract√©ristiques encod√©es en one-hot √† partir du pr√©processeur. Il utilise le pr√©fixe `'cat'` pour identifier les caract√©ristiques cat√©gorielles dans le pr√©processeur.

6. **Combinaison de toutes les caract√©ristiques** :
   - `attributes = vars_num + extra_attribs + cat_ohe_attribs`: Combine les caract√©ristiques num√©riques originales (`vars_num`), les variables suppl√©mentaires et les variables cat√©gorielles encod√©es en one-hot pour former une liste compl√®te de toutes les caract√©ristiques.

7. **Analyse de l'importance des variables** :
   - `importance_result = sorted(zip(feature_importances, attributes), reverse=True)`: Combine les importances des variables avec leurs noms et les trie par ordre d√©croissant d'importance.
   - `imp_df = pd.DataFrame(importance_result, columns=["Score d'importance", "Variable"])`: Cr√©e un DataFrame Pandas √† partir des r√©sultats d'importance des variables.

En r√©sum√©, ce code √©value les performances du mod√®le final sur l'ensemble de test en calculant la RMSE, puis analyse l'importance des variables dans les pr√©dictions du mod√®le en extrayant les coefficients d'importance des variables et en les combinant avec les noms des variables. Il fournit ainsi une analyse d√©taill√©e des facteurs qui influent le plus sur les pr√©dictions du mod√®le.


```python
plt.figure(figsize=(10,16))
sns.barplot(data=imp_df, 
            x="Score d'importance", 
            y="Variable");
```


    
![png](french_real_estate_prediction_files/french_real_estate_prediction_62_0.png)
    



```python
# Enregistrement
import joblib
joblib.dump(final_model, "final_model.pkl")
```




    ['final_model.pkl']


Ce code r√©alise deux actions :

1. **Cr√©ation d'un graphique √† barres pour visualiser l'importance des variables** :
   - `plt.figure(figsize=(10,16))`: Cr√©e une nouvelle figure de matplotlib avec une taille sp√©cifi√©e (10 pouces de largeur par 16 pouces de hauteur).
   - `sns.barplot(data=imp_df, x="Score d'importance", y="Variable")`: Cr√©e un graphique √† barres en utilisant les donn√©es du DataFrame `imp_df`, o√π les variables sont affich√©es sur l'axe des ordonn√©es (`y`) et les scores d'importance sont affich√©s sur l'axe des abscisses (`x`). Cela permet de visualiser l'importance relative des diff√©rentes variables dans les pr√©dictions du mod√®le.

2. **Sauvegarde du mod√®le final entra√Æn√©** :
   - `joblib.dump(final_model, "final_model.pkl")`: Utilise la fonction `dump` du module `joblib` pour sauvegarder le mod√®le final entra√Æn√© (`final_model`) dans un fichier binaire sp√©cifi√© (`final_model.pkl`). Cette op√©ration permet de conserver le mod√®le pour une utilisation future, par exemple pour effectuer des pr√©dictions sur de nouvelles donn√©es sans avoir √† r√©-entra√Æner le mod√®le √† chaque fois.

En r√©sum√©, ce code g√©n√®re un graphique √† barres pour visualiser l'importance des variables dans les pr√©dictions du mod√®le, puis sauvegarde le mod√®le final entra√Æn√© dans un fichier pour une utilisation ult√©rieure. Cela permet d'analyser les r√©sultats du mod√®le et de le d√©ployer facilement dans des applications ou des environnements de production.


üñ• **Conclusion**

Apr√®s avoir construit et sauvegard√© le mod√®le de pr√©diction d'un bien immobilier, plusieurs √©tapes peuvent suivre pour tirer le meilleur parti du mod√®le et l'int√©grer dans des applications r√©elles :

1. **√âvaluation continue du mod√®le** :
   - Il est important de suivre et d'√©valuer r√©guli√®rement les performances du mod√®le pour s'assurer qu'il continue de fournir des pr√©dictions pr√©cises. Cela peut impliquer la surveillance des m√©triques de performance sur des donn√©es de validation ou de test, ainsi que la mise √† jour p√©riodique du mod√®le si n√©cessaire.

2. **D√©ploiement dans un environnement de production** :
   - Une fois que le mod√®le est pr√™t, il peut √™tre d√©ploy√© dans un environnement de production o√π il peut √™tre utilis√© pour faire des pr√©dictions sur de nouvelles donn√©es en temps r√©el. Cela peut se faire en cr√©ant une API web, en int√©grant le mod√®le dans une application ou un service existant, ou en le d√©ployant sur un serveur.

3. **Int√©gration dans des syst√®mes d√©cisionnels** :
   - Le mod√®le peut √™tre int√©gr√© dans des syst√®mes d√©cisionnels ou des outils d'analyse pour aider √† la prise de d√©cision. Par exemple, il peut √™tre utilis√© pour estimer la valeur d'un bien immobilier dans le cadre d'une √©valuation immobili√®re ou pour prendre des d√©cisions d'investissement immobilier.

4. **Formation et support utilisateur** :
   - Il peut √™tre n√©cessaire de former les utilisateurs finaux sur la fa√ßon d'utiliser le mod√®le et de fournir un support technique continu pour r√©pondre √† leurs questions et r√©soudre les probl√®mes √©ventuels.

5. **Gestion des mises √† jour et de la maintenance** :
   - Comme les donn√©es et les besoins commerciaux √©voluent, il peut √™tre n√©cessaire de mettre √† jour et de maintenir le mod√®le en continu. Cela peut impliquer l'ajout de nouvelles fonctionnalit√©s, l'optimisation des hyperparam√®tres, ou la r√©√©valuation de l'ensemble de donn√©es utilis√© pour l'entra√Ænement.

6. **S√©curit√© et confidentialit√© des donn√©es** :
   - Assurer la s√©curit√© et la confidentialit√© des donn√©es est essentiel lors du d√©ploiement d'un mod√®le dans un environnement de production. Cela peut impliquer la mise en place de mesures de s√©curit√© telles que le chiffrement des donn√©es, le contr√¥le d'acc√®s et la gestion des identit√©s, ainsi que le respect des r√©glementations telles que le RGPD.

En r√©sum√©, une fois que le mod√®le de pr√©diction d'un bien immobilier est construit et sauvegard√©, la suite des √©tapes implique principalement son d√©ploiement, son √©valuation continue, son int√©gration dans des syst√®mes existants, la formation des utilisateurs finaux, la gestion des mises √† jour et la s√©curit√© des donn√©es.

**Dans cette application, vous trouverez plusieurs projets d'int√©gration de Mod√®les Machine Learning dans des applications web. Amusez-vous bien :)**